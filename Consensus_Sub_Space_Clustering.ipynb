{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1db54c87ef1cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initial Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddc06e7fedf8ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T12:31:18.897613200Z",
     "start_time": "2023-12-29T12:31:11.321503900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86669f7f9aa2933",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.***[Loading the data]***:\n",
    "For MNIST:\n",
    "In Python using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a603f956639a2628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T12:31:19.214379800Z",
     "start_time": "2023-12-29T12:31:18.897613200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77672e2c3020551d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For CIFAR-10:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T12:31:21.186545200Z",
     "start_time": "2023-12-29T12:31:19.218030700Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations for CIFAR-10\n",
    "transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ce8249d1c488c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Paper results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea59dff45c49e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T12:31:21.257701400Z",
     "start_time": "2023-12-29T12:31:21.190209100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results obtained in the paper:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MNIST ACC</th>\n",
       "      <th>MNIST NMI</th>\n",
       "      <th>USPS ACC</th>\n",
       "      <th>USPS NMI</th>\n",
       "      <th>CIFAR-10 ACC</th>\n",
       "      <th>CIFAR-10 NMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-means</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Cluster</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep K-means</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSC No Flatten</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSC No Filter</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CSC No Voting</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CSC</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Method   MNIST ACC   MNIST NMI   USPS ACC   USPS NMI  CIFAR-10 ACC  \\\n",
       "0         K-means        0.58        0.49       0.48       0.42          0.14   \n",
       "1    Deep Cluster        0.86        0.83       0.67       0.69            NA   \n",
       "2    Deep K-means        0.84        0.80       0.76       0.78            NA   \n",
       "3  CSC No Flatten        0.85        0.79       0.83       0.78          0.12   \n",
       "4   CSC No Filter        0.83        0.76       0.84       0.79          0.14   \n",
       "5   CSC No Voting        0.82        0.77       0.82       0.76          0.14   \n",
       "6             CSC        0.86        0.81       0.83       0.79          0.15   \n",
       "\n",
       "   CIFAR-10 NMI  \n",
       "0          0.12  \n",
       "1            NA  \n",
       "2            NA  \n",
       "3          0.08  \n",
       "4          0.10  \n",
       "5          0.10  \n",
       "6          0.11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'paper_results.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Results obtained in the paper:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521320c9",
   "metadata": {},
   "source": [
    "### A - 1) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716e1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization with min-max method\n",
    "def min_max_scale_dataset(dataset, batch_size=64):\n",
    "    \n",
    "    min_pixel_value = float('inf')\n",
    "    max_pixel_value = float('-inf')\n",
    "\n",
    "    for images, _ in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "        min_pixel_value = min(min_pixel_value, images.min())\n",
    "        max_pixel_value = max(max_pixel_value, images.max())\n",
    "\n",
    "    \n",
    "    min_max_scaler = Normalize(min_pixel_value, max_pixel_value)\n",
    "    # Apply the min-max scaling to the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), min_max_scaler])\n",
    "    normalized_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    normalized_loader = DataLoader(normalized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return normalized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d294bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36cb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = min_max_scale_dataset(mnist_trainset)\n",
    "mnist_test_loader = min_max_scale_dataset(mnist_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f7da262358c24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A - 2) Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48b0a",
   "metadata": {},
   "source": [
    "The autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974bbb9f6e96ade2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T12:31:21.278344300Z",
     "start_time": "2023-12-29T12:31:21.257701400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Linear(16 * 14 * 14, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        flatten_x = x.view(x.size(0), -1)  # Flatten the output from the encoder\n",
    "        bottleneck_features = self.bottleneck(flatten_x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return bottleneck_features, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d38714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels_mnist = 1  # MNIST images are grayscale\n",
    "input_channels_cifar = 3  # CIFAR-10 images have 3 channels (RGB)\n",
    "\n",
    "mnist_autoencoder = ConvAutoencoder(input_channels_mnist)\n",
    "cifar_autoencoder = ConvAutoencoder(input_channels_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd93bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_mnist = torch.optim.Adam(mnist_autoencoder.parameters(), lr=0.001)\n",
    "optimizer_cifar = torch.optim.Adam(cifar_autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4ced",
   "metadata": {},
   "source": [
    "The loop for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1369bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 10\n",
    "def training_loop (model, loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        \n",
    "        for data in loader:\n",
    "            img, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCH}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5afdbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0021\n",
      "Epoch [2/10], Loss: 0.0011\n",
      "Epoch [3/10], Loss: 0.0008\n",
      "Epoch [4/10], Loss: 0.0006\n",
      "Epoch [5/10], Loss: 0.0006\n",
      "Epoch [6/10], Loss: 0.0005\n",
      "Epoch [7/10], Loss: 0.0005\n",
      "Epoch [8/10], Loss: 0.0004\n",
      "Epoch [9/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "mnist_autoencoder = training_loop(mnist_autoencoder,mnist_train_loader,optimizer_mnist)\n",
    "#cifar_autoencoder = training_loop(cifar_autoencoder,cifar_train_loader,optimizer_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c99cc1",
   "metadata": {},
   "source": [
    "The evaluation loop to extract 500 features from the bottleneck layer for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b57c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation (model, loader):\n",
    "    '''\n",
    "    output shape : bottelneck_feature (nb img,500)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    bottleneck_features_array = []\n",
    "    outputs_array = []\n",
    "    total_loss = 0\n",
    " \n",
    "    for data in loader:\n",
    "        img, _ = data\n",
    "       \n",
    "        bottleneck_features, output = model(img)\n",
    "\n",
    "        for features in bottleneck_features:\n",
    "            bottleneck_features_array.append(features.detach().numpy())\n",
    "        outputs_array.append(output.detach().numpy())\n",
    "\n",
    "        loss = criterion(output, img)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    print(\"Evaluation loss : \",average_loss)\n",
    "    \n",
    "    return np.array(bottleneck_features_array), outputs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e7f7005b5d1e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### B - Best features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140d2d8",
   "metadata": {},
   "source": [
    "Do the Non negativ Matrix Factorization (NMF) to estimate a decomposition W*H from V \\\n",
    "Calcul the error rate E from W*H+E = V \\\n",
    "Sort feature by error rate and keep the 50% best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4313031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_filtering_nmf(features, k=1, removal_percentage=50):\n",
    "    '''\n",
    "    output shape : (nb img,250)\n",
    "    '''\n",
    "\n",
    "    #it doesn't work because of negative value so we remove the negative here\n",
    "    min_value = np.min(features)\n",
    "    features_shifted = features - min_value + 1e-10 \n",
    "\n",
    "    # NMF : find the decomposition V=W*H\n",
    "    nmf_model = NMF(n_components=k, init='random', random_state=None)\n",
    "    nmf_features = nmf_model.fit_transform(features_shifted)\n",
    "\n",
    "    #reconstruc W*H to evaluate with V to find E\n",
    "    reconstructed_features = np.dot(nmf_features, nmf_model.components_)\n",
    "    reconstruction_error = np.abs(features - reconstructed_features)\n",
    "\n",
    "    # Sort features by their E\n",
    "    sorted_indices = np.argsort(reconstruction_error, axis=1)\n",
    "    num_features_to_keep = int((100 - removal_percentage) / 100 * features.shape[1])\n",
    "    selected_features = features[np.arange(features.shape[0])[:, None], sorted_indices[:, :num_features_to_keep]]\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(selected_features)\n",
    "\n",
    "    return np.array(standardized_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94783182",
   "metadata": {},
   "source": [
    "### Loop over A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8efd9",
   "metadata": {},
   "source": [
    "do a loop over steps A and B  to increase number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c69341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss :  0.00038035106989167835\n",
      "Evaluation loss :  0.000380320429360357\n",
      "Evaluation loss :  0.000380341081351572\n",
      "Evaluation loss :  0.0003803369247784981\n",
      "Evaluation loss :  0.0003803375105163567\n",
      "Evaluation loss :  0.0003803434074860809\n",
      "Evaluation loss :  0.0003803311711559986\n",
      "Evaluation loss :  0.0003803368395458518\n",
      "Evaluation loss :  0.00038034402512024835\n",
      "Evaluation loss :  0.00038035992083813885\n"
     ]
    }
   ],
   "source": [
    "features_matrix = None\n",
    "\n",
    "for i in range(10):\n",
    "    features,_ = evaluation(mnist_autoencoder,mnist_test_loader)\n",
    "    sorted_features = feature_filtering_nmf(features)\n",
    "    \n",
    "    if i == 0 :\n",
    "        features_matrix = sorted_features\n",
    "    else :\n",
    "        features_matrix = np.concatenate((features_matrix,sorted_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fc113ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(features_matrix.shape)\n",
    "\n",
    "#C'est cette matrice qui doit être utilisé pour l'étape C\n",
    "# sa dimensio est (60000,2500)\n",
    "# 60 000 c'est le nombre d'image passée en input à l'origine\n",
    "# 2500 c'est le nombre de features décrivant chaque image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74355b5",
   "metadata": {},
   "source": [
    "### C - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970854c3293e651b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load MNIST and CIFAR-10 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd43e6362d2fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T12:31:22.795817600Z",
     "start_time": "2023-12-29T12:31:21.262708800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "cifar_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
