{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1db54c87ef1cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initial Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4ddc06e7fedf8ce9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.568218900Z",
     "start_time": "2024-01-03T16:44:02.693234100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86669f7f9aa2933",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Loading the data\n",
    "For MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a603f956639a2628",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.743102100Z",
     "start_time": "2024-01-03T16:44:02.727920500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "desired_length = 1000\n",
    "\n",
    "# Create indices for the subsampled dataset (randomly sampled)\n",
    "indices = list(range(len(mnist_trainset)))\n",
    "subset_indices = random.sample(indices, desired_length)\n",
    "\n",
    "# Create a Subset using the indices\n",
    "mnist_trainset = Subset(mnist_trainset, subset_indices)\n",
    "\n",
    "# Create a DataLoader for the subsampled dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ce8249d1c488c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Paper results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "bea59dff45c49e93",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.805590900Z",
     "start_time": "2024-01-03T16:44:02.798607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results obtained in the paper:\n"
     ]
    },
    {
     "data": {
      "text/plain": "           Method   MNIST ACC   MNIST NMI   USPS ACC   USPS NMI  CIFAR-10 ACC  \\\n0         K-means        0.58        0.49       0.48       0.42          0.14   \n1    Deep Cluster        0.86        0.83       0.67       0.69            NA   \n2    Deep K-means        0.84        0.80       0.76       0.78            NA   \n3  CSC No Flatten        0.85        0.79       0.83       0.78          0.12   \n4   CSC No Filter        0.83        0.76       0.84       0.79          0.14   \n5   CSC No Voting        0.82        0.77       0.82       0.76          0.14   \n6             CSC        0.86        0.81       0.83       0.79          0.15   \n\n   CIFAR-10 NMI  \n0          0.12  \n1            NA  \n2            NA  \n3          0.08  \n4          0.10  \n5          0.10  \n6          0.11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>MNIST ACC</th>\n      <th>MNIST NMI</th>\n      <th>USPS ACC</th>\n      <th>USPS NMI</th>\n      <th>CIFAR-10 ACC</th>\n      <th>CIFAR-10 NMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>K-means</td>\n      <td>0.58</td>\n      <td>0.49</td>\n      <td>0.48</td>\n      <td>0.42</td>\n      <td>0.14</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Deep Cluster</td>\n      <td>0.86</td>\n      <td>0.83</td>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Deep K-means</td>\n      <td>0.84</td>\n      <td>0.80</td>\n      <td>0.76</td>\n      <td>0.78</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CSC No Flatten</td>\n      <td>0.85</td>\n      <td>0.79</td>\n      <td>0.83</td>\n      <td>0.78</td>\n      <td>0.12</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CSC No Filter</td>\n      <td>0.83</td>\n      <td>0.76</td>\n      <td>0.84</td>\n      <td>0.79</td>\n      <td>0.14</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CSC No Voting</td>\n      <td>0.82</td>\n      <td>0.77</td>\n      <td>0.82</td>\n      <td>0.76</td>\n      <td>0.14</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CSC</td>\n      <td>0.86</td>\n      <td>0.81</td>\n      <td>0.83</td>\n      <td>0.79</td>\n      <td>0.15</td>\n      <td>0.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'paper_results.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Results obtained in the paper:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521320c9",
   "metadata": {},
   "source": [
    "### A - 1) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "716e1287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.810607500Z",
     "start_time": "2024-01-03T16:44:02.830455200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data normalization with min-max method\n",
    "def min_max_scale_dataset(dataset, batch_size=64):\n",
    "    \n",
    "    min_pixel_value = float('inf')\n",
    "    max_pixel_value = float('-inf')\n",
    "\n",
    "    for images, _ in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "        min_pixel_value = min(min_pixel_value, images.min())\n",
    "        max_pixel_value = max(max_pixel_value, images.max())\n",
    "\n",
    "    \n",
    "    min_max_scaler = Normalize(min_pixel_value, max_pixel_value)\n",
    "    # Apply the min-max scaling to the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), min_max_scaler])\n",
    "    normalized_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    #select only 1000 datapoints\n",
    "    desired_length = 1000\n",
    "    indices = list(range(len(normalized_dataset)))\n",
    "    subset_indices = random.sample(indices, desired_length)\n",
    "    subset = Subset(normalized_dataset, subset_indices)\n",
    "\n",
    "    normalized_loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return normalized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d294bff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.810607500Z",
     "start_time": "2024-01-03T16:44:02.846491900Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d36cb56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.040955800Z",
     "start_time": "2024-01-03T16:44:02.862129300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "mnist_train_loader = min_max_scale_dataset(mnist_trainset)\n",
    "mnist_test_loader = min_max_scale_dataset(mnist_testset)\n",
    "\n",
    "print(len(mnist_train_loader))\n",
    "print(len(mnist_test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f7da262358c24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A - 2) Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48b0a",
   "metadata": {},
   "source": [
    "The autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "974bbb9f6e96ade2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.040955800Z",
     "start_time": "2024-01-03T16:44:06.024801Z"
    }
   },
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Linear(16 * 14 * 14, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        flatten_x = x.view(x.size(0), -1)  # Flatten the output from the encoder\n",
    "        bottleneck_features = self.bottleneck(flatten_x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return bottleneck_features, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9d38714f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.151947400Z",
     "start_time": "2024-01-03T16:44:06.040955800Z"
    }
   },
   "outputs": [],
   "source": [
    "input_channels_mnist = 1  # MNIST images are grayscale\n",
    "input_channels_cifar = 3  # CIFAR-10 images have 3 channels (RGB)\n",
    "\n",
    "mnist_autoencoder = ConvAutoencoder(input_channels_mnist)\n",
    "cifar_autoencoder = ConvAutoencoder(input_channels_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cd93bd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.168044900Z",
     "start_time": "2024-01-03T16:44:06.122035400Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_mnist = torch.optim.Adam(mnist_autoencoder.parameters(), lr=0.001)\n",
    "optimizer_cifar = torch.optim.Adam(cifar_autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4ced",
   "metadata": {},
   "source": [
    "The loop for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a1369bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.199309300Z",
     "start_time": "2024-01-03T16:44:06.136044900Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 10\n",
    "def training_loop (model, loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        \n",
    "        for data in loader:\n",
    "            img, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCH}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c5afdbae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:11.034185200Z",
     "start_time": "2024-01-03T16:44:06.153034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1985\n",
      "Epoch [2/10], Loss: 0.1842\n",
      "Epoch [3/10], Loss: 0.1691\n",
      "Epoch [4/10], Loss: 0.1531\n",
      "Epoch [5/10], Loss: 0.1375\n",
      "Epoch [6/10], Loss: 0.1209\n",
      "Epoch [7/10], Loss: 0.1062\n",
      "Epoch [8/10], Loss: 0.0915\n",
      "Epoch [9/10], Loss: 0.0787\n",
      "Epoch [10/10], Loss: 0.0679\n"
     ]
    }
   ],
   "source": [
    "mnist_autoencoder = training_loop(mnist_autoencoder,mnist_train_loader,optimizer_mnist)\n",
    "#cifar_autoencoder = training_loop(cifar_autoencoder,cifar_train_loader,optimizer_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c99cc1",
   "metadata": {},
   "source": [
    "# The evaluation loop to extract 500 features from the bottleneck layer for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "6b57c362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:11.063486700Z",
     "start_time": "2024-01-03T16:44:11.046272700Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation (model, loader):\n",
    "    '''\n",
    "    output shape : bottelneck_feature (nb img,500)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    bottleneck_features_array = []\n",
    "    outputs_array = []\n",
    "    total_loss = 0\n",
    " \n",
    "    for data in loader:\n",
    "        img, _ = data\n",
    "       \n",
    "        bottleneck_features, output = model(img)\n",
    "\n",
    "        for features in bottleneck_features:\n",
    "            bottleneck_features_array.append(features.detach().numpy())\n",
    "        outputs_array.append(output.detach().numpy())\n",
    "\n",
    "        loss = criterion(output, img)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    print(\"Evaluation loss : \",average_loss)\n",
    "    \n",
    "    return np.array(bottleneck_features_array), outputs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e7f7005b5d1e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### B - Best features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140d2d8",
   "metadata": {},
   "source": [
    "Do the Non negativ Matrix Factorization (NMF) to estimate a decomposition W*H from V \\\n",
    "Calcul the error rate E from W*H+E = V \\\n",
    "Sort feature by error rate and keep the 50% best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4313031f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:11.093991Z",
     "start_time": "2024-01-03T16:44:11.050560300Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_filtering_nmf(features, k=1, removal_percentage=50):\n",
    "    '''\n",
    "    output shape : (nb img,250)\n",
    "    '''\n",
    "\n",
    "    #it doesn't work because of negative value so we remove the negative here\n",
    "    min_value = np.min(features)\n",
    "    features_shifted = features - min_value + 1e-10 \n",
    "\n",
    "    # NMF : find the decomposition V=W*H\n",
    "    nmf_model = NMF(n_components=k, init='random', random_state=None)\n",
    "    nmf_features = nmf_model.fit_transform(features_shifted)\n",
    "\n",
    "    #reconstruc W*H to evaluate with V to find E\n",
    "    reconstructed_features = np.dot(nmf_features, nmf_model.components_)\n",
    "    reconstruction_error = np.abs(features - reconstructed_features)\n",
    "\n",
    "    # Sort features by their E\n",
    "    sorted_indices = np.argsort(reconstruction_error, axis=1)\n",
    "    num_features_to_keep = int((100 - removal_percentage) / 100 * features.shape[1])\n",
    "    selected_features = features[np.arange(features.shape[0])[:, None], sorted_indices[:, :num_features_to_keep]]\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(selected_features)\n",
    "\n",
    "    return np.array(standardized_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94783182",
   "metadata": {},
   "source": [
    "### Loop over A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8efd9",
   "metadata": {},
   "source": [
    "do a loop over steps A and B  to increase number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "27c69341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.005586600Z",
     "start_time": "2024-01-03T16:44:11.063486700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss :  0.0669815381988883\n",
      "Evaluation loss :  0.06697776727378368\n",
      "Evaluation loss :  0.06697895145043731\n",
      "Evaluation loss :  0.06697568064555526\n",
      "Evaluation loss :  0.06697293743491173\n",
      "Evaluation loss :  0.06697092670947313\n",
      "Evaluation loss :  0.06696992041543126\n",
      "Evaluation loss :  0.06697973143309355\n",
      "Evaluation loss :  0.06697160750627518\n",
      "Evaluation loss :  0.06697543803602457\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-2.1768034 , -0.7672216 , -1.9268775 , ...,  1.4528033 ,\n        -0.767327  , -1.222267  ],\n       [ 0.60915107,  0.9865285 ,  0.30301556, ..., -0.04925562,\n         0.42775097, -0.99824804],\n       [ 0.78530747, -0.3182277 , -0.10890191, ..., -0.5416779 ,\n         2.6710598 ,  1.6967471 ],\n       ...,\n       [ 1.1935655 ,  1.7844521 ,  0.7646519 , ...,  0.39516893,\n        -1.4274007 ,  0.96344304],\n       [ 1.5093113 , -2.1395435 , -1.5771767 , ...,  0.13919914,\n        -0.7725521 , -0.46434966],\n       [-0.89396626, -1.5922662 , -1.2105101 , ..., -0.5993205 ,\n         0.05992774,  0.45458996]], dtype=float32)"
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix = None\n",
    "\n",
    "for i in range(10):\n",
    "    features,_ = evaluation(mnist_autoencoder,mnist_test_loader)\n",
    "    sorted_features = feature_filtering_nmf(features)\n",
    "    \n",
    "    if i == 0 :\n",
    "        features_matrix = sorted_features\n",
    "    else :\n",
    "        features_matrix = np.concatenate((features_matrix,sorted_features),axis=1)\n",
    "        \n",
    "features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74355b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### C - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbaec17cae500b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03a52553f222d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "36a3d2d118485b21",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.044849200Z",
     "start_time": "2024-01-03T16:44:16.005586600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b547df21df315",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "38c2db1ed18a3a73",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.047360Z",
     "start_time": "2024-01-03T16:44:16.023987500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()  # Adding a sigmoid activation function\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))  # Applying sigmoid activation\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "3dd4e1990265109a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.078781900Z",
     "start_time": "2024-01-03T16:44:16.032834100Z"
    }
   },
   "outputs": [],
   "source": [
    "class SecondDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(SecondDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 2 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5be6eff65c6981e1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.078781900Z",
     "start_time": "2024-01-03T16:44:16.047360Z"
    }
   },
   "outputs": [],
   "source": [
    "class ThirdDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(ThirdDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.leaky_relu(self.fc1(z))\n",
    "        z = self.leaky_relu(self.fc2(z))\n",
    "        z = self.leaky_relu(self.fc3(z))\n",
    "        reconstruction = self.tanh(self.fc4(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc330de69b2fabd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c200a1da49e97f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create instances of each encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e64a6baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.139203100Z",
     "start_time": "2024-01-03T16:44:16.068289200Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2500\n",
    "hidden_dim = 500\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "17adb35de0caf903",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.214978900Z",
     "start_time": "2024-01-03T16:44:16.086696500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Decoder(\n   (fc2): Linear(in_features=100, out_features=500, bias=True)\n   (fc3): Linear(in_features=500, out_features=2500, bias=True)\n   (sigmoid): Sigmoid()\n ),\n SecondDecoder(\n   (fc1): Linear(in_features=100, out_features=1000, bias=True)\n   (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n   (fc3): Linear(in_features=1000, out_features=2500, bias=True)\n   (relu): ReLU()\n   (sigmoid): Sigmoid()\n ),\n ThirdDecoder(\n   (fc1): Linear(in_features=100, out_features=2000, bias=True)\n   (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n   (fc3): Linear(in_features=1000, out_features=500, bias=True)\n   (fc4): Linear(in_features=500, out_features=2500, bias=True)\n   (leaky_relu): LeakyReLU(negative_slope=0.01)\n   (tanh): Tanh()\n )]"
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "secondDecoder = SecondDecoder(latent_dim, hidden_dim,input_dim)\n",
    "thirdDecoder = ThirdDecoder(latent_dim, hidden_dim,input_dim)\n",
    "\n",
    "\n",
    "# Put all encoders into a list\n",
    "all_decoders = [decoder, secondDecoder, thirdDecoder]\n",
    "all_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5cef2933ff119699",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.240791Z",
     "start_time": "2024-01-03T16:44:16.219191400Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, all_decoders):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, hidden_dim, input_dim)\n",
    "        self.decoders = nn.ModuleList(all_decoders)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        reconstructions = []\n",
    "        for decoder in self.decoders:\n",
    "            reconstruction = decoder(z)\n",
    "            reconstructions.append(reconstruction)\n",
    "\n",
    "        return reconstructions, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc50f7f645c43b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "f3047d0a09085996",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.234938200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VAE(\n  (encoder): Encoder(\n    (fc1): Linear(in_features=2500, out_features=500, bias=True)\n    (fc_mu): Linear(in_features=500, out_features=100, bias=True)\n    (fc_logvar): Linear(in_features=500, out_features=100, bias=True)\n  )\n  (decoders): ModuleList(\n    (0): Decoder(\n      (fc2): Linear(in_features=100, out_features=500, bias=True)\n      (fc3): Linear(in_features=500, out_features=2500, bias=True)\n      (sigmoid): Sigmoid()\n    )\n    (1): SecondDecoder(\n      (fc1): Linear(in_features=100, out_features=1000, bias=True)\n      (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n      (fc3): Linear(in_features=1000, out_features=2500, bias=True)\n      (relu): ReLU()\n      (sigmoid): Sigmoid()\n    )\n    (2): ThirdDecoder(\n      (fc1): Linear(in_features=100, out_features=2000, bias=True)\n      (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n      (fc3): Linear(in_features=1000, out_features=500, bias=True)\n      (fc4): Linear(in_features=500, out_features=2500, bias=True)\n      (leaky_relu): LeakyReLU(negative_slope=0.01)\n      (tanh): Tanh()\n    )\n  )\n)"
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(all_decoders)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629b79e113ec555",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8bf66b533d875023",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.273579900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data to be in the [0, 1] range\n",
    "max_val = features_matrix.max()\n",
    "min_val = features_matrix.min()\n",
    "normalized_features_matrix = (features_matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "features_tensor = torch.Tensor(normalized_features_matrix)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(features_tensor, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db7c9d",
   "metadata": {},
   "source": [
    "Dimension of data_loader : [938,1,64,2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1328f5e9312a82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the loss function (using the reconstruction loss and the KL divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c4cf9a2bde7e4a33",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.310310800Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #print(f\"recon_x shape: {recon_x.shape}, x shape: {x.shape}\")\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee539886f49a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "98a8f9600fb1c972",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.323123600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)"
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ded01161eb1ddb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e0bce3aa039323c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.324616Z",
     "start_time": "2024-01-03T16:44:16.336977300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1723.1172\n",
      "Epoch [2/10], Loss: 1713.7711\n",
      "Epoch [3/10], Loss: 1711.7840\n",
      "Epoch [4/10], Loss: 1710.7201\n",
      "Epoch [5/10], Loss: 1709.8676\n",
      "Epoch [6/10], Loss: 1709.3194\n",
      "Epoch [7/10], Loss: 1708.9721\n",
      "Epoch [8/10], Loss: 1708.6248\n",
      "Epoch [9/10], Loss: 1708.3969\n",
      "Epoch [10/10], Loss: 1708.2492\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructions, mu, logvar = model(batch)\n",
    "\n",
    "        # Calculate the loss using your custom loss function\n",
    "        loss = loss_function(reconstructions[0], batch, mu, logvar)  # Assuming using the first decoder for loss calculation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "398a62c811edb4a0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.684676200Z",
     "start_time": "2024-01-03T16:44:24.324616Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "final_representations = []\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in data_loader:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs, _, _ = model(batch)\n",
    "    \n",
    "    # Stack the representations along a new dimension (dim=1)\n",
    "    representations = torch.stack(outputs, dim=1)\n",
    "\n",
    "    # Append the representations to the final list\n",
    "    final_representations.append(representations)\n",
    "\n",
    "# Concatenate the representations along the batch dimension (dim=0)\n",
    "final_representations = torch.cat(final_representations, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "c69090de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.696642800Z",
     "start_time": "2024-01-03T16:44:24.685718900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "897bd7fef233b5e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### D- Basic Subspace Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7da16f4e8963c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute the similarity matrix using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b221ad65c8da1d81",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.830289300Z",
     "start_time": "2024-01-03T16:44:24.696642800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "(3, 1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "#J'ai pas assez de ram sur mon pc mdr !!!! Ducoup je fais un PCA mais si ona le pc pour sa devrai fonctioner. PS: IL faut au loin 14GB de ram mdr donc un pc qui a 32GB de ram pour le tourner.\n",
    "\n",
    "similarity_list = []\n",
    "for i in range(3):\n",
    "    mat = final_representations[:,i,:]\n",
    "    similarity_matrix = cosine_similarity(mat)\n",
    "    print(similarity_matrix.shape)\n",
    "    similarity_list.append(similarity_matrix)\n",
    "\n",
    "similarity_list = np.array(similarity_list)\n",
    "print(similarity_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b40c915eff8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate the graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e61eb5403d21469",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:26.926056700Z",
     "start_time": "2024-01-03T16:44:24.831257600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1000, 1000)"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_list = []\n",
    "for similarity_matrix in similarity_list :\n",
    "    degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n",
    "    laplacian_matrix = degree_matrix - similarity_matrix\n",
    "    normalized_laplacian = np.dot(np.dot(np.sqrt(np.linalg.inv(degree_matrix)), laplacian_matrix), np.sqrt(np.linalg.inv(degree_matrix)))\n",
    "    laplacian_list.append(normalized_laplacian)\n",
    "\n",
    "laplacian_list = np.array(laplacian_list)\n",
    "laplacian_list.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed266b3d289bd58",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a91f79ac8670e2fc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:28.535538400Z",
     "start_time": "2024-01-03T16:44:26.924181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(30, 1000)"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_eigen_vector_list = []\n",
    "for i in range(len(laplacian_list)):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian_list[i])\n",
    "    k = 10\n",
    "    list_k = []\n",
    "    for j in range(k): \n",
    "        k_eigenvectors = eigenvectors[:, j] / np.linalg.norm(eigenvectors[:, j])\n",
    "        list_k.append(k_eigenvectors)\n",
    "    k_eigen_vector_list.append(list_k)\n",
    "k_eigen_vector_list = np.array(k_eigen_vector_list)\n",
    "print(k_eigen_vector_list.shape)\n",
    "k_eigenvectors_matrix = np.reshape(k_eigen_vector_list,(30,1000))\n",
    "k_eigenvectors_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab9234001e7cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Perform K-Means clustering on selected eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e54ea6cc938de703"
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum R ratio: 0.06781411916017532 achieved with 11 clusters\n"
     ]
    }
   ],
   "source": [
    "max_r_ratio = float('-inf')\n",
    "best_r_ratio_clusters = None\n",
    "\n",
    "for i in range(2, 30):  # Starting from 2 clusters as a minimum\n",
    "    spectral_model = SpectralClustering(n_clusters=i, affinity='nearest_neighbors')\n",
    "    pseudo_labels = spectral_model.fit_predict(k_eigenvectors_matrix)\n",
    "    \n",
    "    # Calculate silhouette score as an example metric\n",
    "    silhouette = silhouette_score(k_eigenvectors_matrix, pseudo_labels)\n",
    "    if silhouette > max_r_ratio:\n",
    "        max_r_ratio = silhouette\n",
    "        best_r_ratio_clusters = i\n",
    "\n",
    "print(f\"Maximum R ratio: {max_r_ratio} achieved with {best_r_ratio_clusters} clusters\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:30.706315Z",
     "start_time": "2024-01-03T16:44:28.532013900Z"
    }
   },
   "id": "5f3316c51061aa2f"
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:30.801394800Z",
     "start_time": "2024-01-03T16:44:30.706315Z"
    }
   },
   "id": "af174cdaccd6754f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
