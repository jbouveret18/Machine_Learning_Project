{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1db54c87ef1cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initial Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddc06e7fedf8ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T11:44:52.663667300Z",
     "start_time": "2024-01-03T11:44:30.415238200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86669f7f9aa2933",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.***[Loading the data]***:\n",
    "For MNIST:\n",
    "In Python using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a603f956639a2628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T11:44:59.022967400Z",
     "start_time": "2024-01-03T11:44:58.342856Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77672e2c3020551d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For CIFAR-10:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:05:50.532060600Z",
     "start_time": "2024-01-02T15:05:47.316312600Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations for CIFAR-10\n",
    "transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ce8249d1c488c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Paper results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea59dff45c49e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:05:50.608466300Z",
     "start_time": "2024-01-02T15:05:50.535292500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results obtained in the paper:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MNIST ACC</th>\n",
       "      <th>MNIST NMI</th>\n",
       "      <th>USPS ACC</th>\n",
       "      <th>USPS NMI</th>\n",
       "      <th>CIFAR-10 ACC</th>\n",
       "      <th>CIFAR-10 NMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-means</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Cluster</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep K-means</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSC No Flatten</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSC No Filter</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CSC No Voting</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CSC</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Method   MNIST ACC   MNIST NMI   USPS ACC   USPS NMI  CIFAR-10 ACC  \\\n",
       "0         K-means        0.58        0.49       0.48       0.42          0.14   \n",
       "1    Deep Cluster        0.86        0.83       0.67       0.69            NA   \n",
       "2    Deep K-means        0.84        0.80       0.76       0.78            NA   \n",
       "3  CSC No Flatten        0.85        0.79       0.83       0.78          0.12   \n",
       "4   CSC No Filter        0.83        0.76       0.84       0.79          0.14   \n",
       "5   CSC No Voting        0.82        0.77       0.82       0.76          0.14   \n",
       "6             CSC        0.86        0.81       0.83       0.79          0.15   \n",
       "\n",
       "   CIFAR-10 NMI  \n",
       "0          0.12  \n",
       "1            NA  \n",
       "2            NA  \n",
       "3          0.08  \n",
       "4          0.10  \n",
       "5          0.10  \n",
       "6          0.11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'paper_results.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Results obtained in the paper:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521320c9",
   "metadata": {},
   "source": [
    "### A - 1) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716e1287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:05:50.624459500Z",
     "start_time": "2024-01-02T15:05:50.567379200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data normalization with min-max method\n",
    "def min_max_scale_dataset(dataset, batch_size=64):\n",
    "    \n",
    "    min_pixel_value = float('inf')\n",
    "    max_pixel_value = float('-inf')\n",
    "\n",
    "    for images, _ in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "        min_pixel_value = min(min_pixel_value, images.min())\n",
    "        max_pixel_value = max(max_pixel_value, images.max())\n",
    "\n",
    "    \n",
    "    min_max_scaler = Normalize(min_pixel_value, max_pixel_value)\n",
    "    # Apply the min-max scaling to the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), min_max_scaler])\n",
    "    normalized_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    normalized_loader = DataLoader(normalized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return normalized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d294bff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:05:50.687259300Z",
     "start_time": "2024-01-02T15:05:50.583822600Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36cb56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:06:05.953559500Z",
     "start_time": "2024-01-02T15:05:50.600255600Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_train_loader = min_max_scale_dataset(mnist_trainset)\n",
    "mnist_test_loader = min_max_scale_dataset(mnist_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f7da262358c24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A - 2) Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48b0a",
   "metadata": {},
   "source": [
    "The autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974bbb9f6e96ade2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:06:05.970605400Z",
     "start_time": "2024-01-02T15:06:05.953559500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Linear(16 * 14 * 14, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        flatten_x = x.view(x.size(0), -1)  # Flatten the output from the encoder\n",
    "        bottleneck_features = self.bottleneck(flatten_x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return bottleneck_features, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d38714f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:06:06.041078600Z",
     "start_time": "2024-01-02T15:06:05.970605400Z"
    }
   },
   "outputs": [],
   "source": [
    "input_channels_mnist = 1  # MNIST images are grayscale\n",
    "input_channels_cifar = 3  # CIFAR-10 images have 3 channels (RGB)\n",
    "\n",
    "mnist_autoencoder = ConvAutoencoder(input_channels_mnist)\n",
    "cifar_autoencoder = ConvAutoencoder(input_channels_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd93bd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:06:06.064809500Z",
     "start_time": "2024-01-02T15:06:06.035568100Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_mnist = torch.optim.Adam(mnist_autoencoder.parameters(), lr=0.001)\n",
    "optimizer_cifar = torch.optim.Adam(cifar_autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4ced",
   "metadata": {},
   "source": [
    "The loop for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1369bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:06:06.069786800Z",
     "start_time": "2024-01-02T15:06:06.056376800Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 10\n",
    "def training_loop (model, loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        \n",
    "        for data in loader:\n",
    "            img, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCH}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5afdbae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:10:06.030197700Z",
     "start_time": "2024-01-02T15:06:06.069786800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0023\n",
      "Epoch [2/10], Loss: 0.0012\n",
      "Epoch [3/10], Loss: 0.0008\n",
      "Epoch [4/10], Loss: 0.0007\n",
      "Epoch [5/10], Loss: 0.0006\n",
      "Epoch [6/10], Loss: 0.0006\n",
      "Epoch [7/10], Loss: 0.0005\n",
      "Epoch [8/10], Loss: 0.0004\n",
      "Epoch [9/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "mnist_autoencoder = training_loop(mnist_autoencoder,mnist_train_loader,optimizer_mnist)\n",
    "#cifar_autoencoder = training_loop(cifar_autoencoder,cifar_train_loader,optimizer_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c99cc1",
   "metadata": {},
   "source": [
    "# The evaluation loop to extract 500 features from the bottleneck layer for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b57c362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:10:06.040395300Z",
     "start_time": "2024-01-02T15:10:06.012522300Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation (model, loader):\n",
    "    '''\n",
    "    output shape : bottelneck_feature (nb img,500)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    bottleneck_features_array = []\n",
    "    outputs_array = []\n",
    "    total_loss = 0\n",
    " \n",
    "    for data in loader:\n",
    "        img, _ = data\n",
    "       \n",
    "        bottleneck_features, output = model(img)\n",
    "\n",
    "        for features in bottleneck_features:\n",
    "            bottleneck_features_array.append(features.detach().numpy())\n",
    "        outputs_array.append(output.detach().numpy())\n",
    "\n",
    "        loss = criterion(output, img)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    print(\"Evaluation loss : \",average_loss)\n",
    "    \n",
    "    return np.array(bottleneck_features_array), outputs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e7f7005b5d1e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### B - Best features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140d2d8",
   "metadata": {},
   "source": [
    "Do the Non negativ Matrix Factorization (NMF) to estimate a decomposition W*H from V \\\n",
    "Calcul the error rate E from W*H+E = V \\\n",
    "Sort feature by error rate and keep the 50% best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4313031f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:10:06.125275100Z",
     "start_time": "2024-01-02T15:10:06.040395300Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_filtering_nmf(features, k=1, removal_percentage=50):\n",
    "    '''\n",
    "    output shape : (nb img,250)\n",
    "    '''\n",
    "\n",
    "    #it doesn't work because of negative value so we remove the negative here\n",
    "    min_value = np.min(features)\n",
    "    features_shifted = features - min_value + 1e-10 \n",
    "\n",
    "    # NMF : find the decomposition V=W*H\n",
    "    nmf_model = NMF(n_components=k, init='random', random_state=None)\n",
    "    nmf_features = nmf_model.fit_transform(features_shifted)\n",
    "\n",
    "    #reconstruc W*H to evaluate with V to find E\n",
    "    reconstructed_features = np.dot(nmf_features, nmf_model.components_)\n",
    "    reconstruction_error = np.abs(features - reconstructed_features)\n",
    "\n",
    "    # Sort features by their E\n",
    "    sorted_indices = np.argsort(reconstruction_error, axis=1)\n",
    "    num_features_to_keep = int((100 - removal_percentage) / 100 * features.shape[1])\n",
    "    selected_features = features[np.arange(features.shape[0])[:, None], sorted_indices[:, :num_features_to_keep]]\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(selected_features)\n",
    "\n",
    "    return np.array(standardized_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94783182",
   "metadata": {},
   "source": [
    "### Loop over A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8efd9",
   "metadata": {},
   "source": [
    "do a loop over steps A and B  to increase number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c69341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:14:11.805669800Z",
     "start_time": "2024-01-02T15:10:06.055730700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss :  0.00040536269625978495\n",
      "Evaluation loss :  0.0004053691415171296\n",
      "Evaluation loss :  0.00040537626463309256\n",
      "Evaluation loss :  0.0004053632352391441\n",
      "Evaluation loss :  0.00040537026440372255\n",
      "Evaluation loss :  0.00040539845180482284\n",
      "Evaluation loss :  0.0004053923176295722\n",
      "Evaluation loss :  0.00040538684762979566\n",
      "Evaluation loss :  0.00040536447814025327\n",
      "Evaluation loss :  0.0004053683726857704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.44056824, -0.6664303 , -1.2171292 , ...,  1.7724246 ,\n",
       "        -1.1947113 , -1.1318264 ],\n",
       "       [-0.4642176 , -0.9276542 ,  0.00310229, ...,  0.07770843,\n",
       "         0.4247359 , -0.247903  ],\n",
       "       [-0.19695908,  0.12700014,  0.7440619 , ..., -0.10283734,\n",
       "        -0.15258826,  1.3201374 ],\n",
       "       ...,\n",
       "       [-1.2641542 , -1.7051498 , -1.669919  , ...,  1.5521952 ,\n",
       "        -0.36662388,  0.07470288],\n",
       "       [ 0.38659778,  1.3508781 ,  1.1443588 , ...,  1.1744195 ,\n",
       "        -1.3440082 , -1.4649853 ],\n",
       "       [-0.2166113 ,  2.2095265 , -0.718271  , ..., -1.7769893 ,\n",
       "        -1.2952588 , -0.48391187]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix = None\n",
    "\n",
    "for i in range(10):\n",
    "    features,_ = evaluation(mnist_autoencoder,mnist_test_loader)\n",
    "    sorted_features = feature_filtering_nmf(features)\n",
    "    \n",
    "    if i == 0 :\n",
    "        features_matrix = sorted_features\n",
    "    else :\n",
    "        features_matrix = np.concatenate((features_matrix,sorted_features),axis=1)\n",
    "        \n",
    "features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74355b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### C - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbaec17cae500b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. # Define the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03a52553f222d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a3d2d118485b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T11:44:21.152092Z",
     "start_time": "2024-01-03T11:44:20.081692300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b547df21df315",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38c2db1ed18a3a73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:00:22.136192100Z",
     "start_time": "2024-01-02T16:00:22.126043700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()  # Adding a sigmoid activation function\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))  # Applying sigmoid activation\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dd4e1990265109a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SecondDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(SecondDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 2 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5be6eff65c6981e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ThirdDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(ThirdDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.leaky_relu(self.fc1(z))\n",
    "        z = self.leaky_relu(self.fc2(z))\n",
    "        z = self.leaky_relu(self.fc3(z))\n",
    "        reconstruction = self.tanh(self.fc4(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc330de69b2fabd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c200a1da49e97f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create instances of each encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e64a6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2500\n",
    "hidden_dim = 500\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17adb35de0caf903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:00:25.227033200Z",
     "start_time": "2024-01-02T16:00:25.115736900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "secondDecoder = SecondDecoder(latent_dim, hidden_dim,input_dim)\n",
    "thirdDecoder = ThirdDecoder(latent_dim, hidden_dim,input_dim)\n",
    "\n",
    "\n",
    "# Put all encoders into a list\n",
    "all_decoders = [decoder, secondDecoder, thirdDecoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cef2933ff119699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:05:20.294050700Z",
     "start_time": "2024-01-02T16:05:20.229394500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, all_decoders):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, hidden_dim, input_dim)\n",
    "        self.decoders = nn.ModuleList(all_decoders)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        reconstructions = []\n",
    "        for decoder in self.decoders:\n",
    "            reconstruction = decoder(z)\n",
    "            reconstructions.append(reconstruction)\n",
    "\n",
    "        return reconstructions, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc50f7f645c43b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3047d0a09085996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:05:23.036778100Z",
     "start_time": "2024-01-02T16:05:22.948437600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (fc1): Linear(in_features=2500, out_features=500, bias=True)\n",
       "    (fc_mu): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (fc_logvar): Linear(in_features=500, out_features=100, bias=True)\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (fc2): Linear(in_features=100, out_features=500, bias=True)\n",
       "      (fc3): Linear(in_features=500, out_features=2500, bias=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): SecondDecoder(\n",
       "      (fc1): Linear(in_features=100, out_features=1000, bias=True)\n",
       "      (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "      (fc3): Linear(in_features=1000, out_features=2500, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): ThirdDecoder(\n",
       "      (fc1): Linear(in_features=100, out_features=2000, bias=True)\n",
       "      (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "      (fc3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "      (fc4): Linear(in_features=500, out_features=2500, bias=True)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(all_decoders)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629b79e113ec555",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. # Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8bf66b533d875023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:30:51.340135600Z",
     "start_time": "2024-01-02T16:30:28.584844500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the data to be in the [0, 1] range\n",
    "max_val = features_matrix.max()\n",
    "min_val = features_matrix.min()\n",
    "normalized_features_matrix = (features_matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "features_tensor = torch.Tensor(normalized_features_matrix)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(features_tensor, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db7c9d",
   "metadata": {},
   "source": [
    "Dimension of data_loader : [938,1,64,2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1328f5e9312a82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. # Define the loss function (using the reconstruction loss and the KL divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4cf9a2bde7e4a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:30:58.961692700Z",
     "start_time": "2024-01-02T16:30:58.914632500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #print(f\"recon_x shape: {recon_x.shape}, x shape: {x.shape}\")\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee539886f49a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. # Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98a8f9600fb1c972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:31:01.276338400Z",
     "start_time": "2024-01-02T16:31:01.252825Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ded01161eb1ddb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. # Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb843fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print (len(data_loader))\n",
    "for a,b in enumerate(data_loader):\n",
    "    print(type(b))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0bce3aa039323c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:36:57.987790200Z",
     "start_time": "2024-01-02T16:31:06.693538400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1724.3300\n",
      "Epoch [2/10], Loss: 1724.3079\n",
      "Epoch [3/10], Loss: 1724.2952\n",
      "Epoch [4/10], Loss: 1724.2835\n",
      "Epoch [5/10], Loss: 1724.2786\n",
      "Epoch [6/10], Loss: 1724.2734\n",
      "Epoch [7/10], Loss: 1724.2713\n",
      "Epoch [8/10], Loss: 1724.2698\n",
      "Epoch [9/10], Loss: 1724.2654\n",
      "Epoch [10/10], Loss: 1724.2611\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructions, mu, logvar = model(batch)\n",
    "\n",
    "        # Calculate the loss using your custom loss function\n",
    "        loss = loss_function(reconstructions[1], batch, mu, logvar)  # Assuming using the first decoder for loss calculation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "398a62c811edb4a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:49:32.735381900Z",
     "start_time": "2024-01-02T17:49:28.993690700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "final_representations = []\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in data_loader:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs, _, _ = model(batch)\n",
    "    \n",
    "    # Stack the representations along a new dimension (dim=1)\n",
    "    representations = torch.stack(outputs, dim=1)\n",
    "\n",
    "    # Append the representations to the final list\n",
    "    final_representations.append(representations)\n",
    "\n",
    "# Concatenate the representations along the batch dimension (dim=0)\n",
    "final_representations = torch.cat(final_representations, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c69090de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 2500])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "897bd7fef233b5e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### D- Basic Subspace Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7da16f4e8963c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute the similarity matrix using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b221ad65c8da1d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T18:17:42.432617700Z",
     "start_time": "2024-01-02T18:17:42.120806500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#J'ai pas assez de ram sur mon pc mdr !!!! Ducoup je fais un PCA mais si ona le pc pour sa devrai fonctioner. PS: IL faut au loin 14GB de ram mdr donc un pc qui a 32GB de ram pour le tourner.\n",
    "similarity_list = []\n",
    "for i in range(3):\n",
    "    mat = final_representations[:,i,:]\n",
    "    similarity_matrix = cosine_similarity(mat)\n",
    "    print(similarity_matrix.shape)\n",
    "    similarity_list.append(similarity_matrix)\n",
    "\n",
    "similarity_list = np.array(similarity_list)\n",
    "print(similarity_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94742392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a7b3cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60000, 60000)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8df1b40c915eff8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate the graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61eb5403d21469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:57:42.810583200Z",
     "start_time": "2024-01-02T17:57:42.687653Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (axis=int, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: axis, out\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[191], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m degree_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m laplacian_matrix \u001b[38;5;241m=\u001b[39m degree_matrix \u001b[38;5;241m-\u001b[39m similarity_matrix\n\u001b[0;32m      3\u001b[0m normalized_laplacian \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(degree_matrix)), laplacian_matrix), np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(degree_matrix)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (axis=int, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: axis, out\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n",
    "laplacian_matrix = degree_matrix - similarity_matrix\n",
    "normalized_laplacian = np.dot(np.dot(np.sqrt(np.linalg.inv(degree_matrix)), laplacian_matrix), np.sqrt(np.linalg.inv(degree_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed266b3d289bd58",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f79ac8670e2fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "k = 10  # Example: Selecting top k eigenvectors\n",
    "k_eigenvectors = eigenvectors[:, :k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab9234001e7cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Perform K-Means clustering on selected eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25168b197e0ac2dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = 5  # Example: Number of clusters\n",
    "spectral_model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors')\n",
    "pseudo_labels = spectral_model.fit_predict(k_eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa2549fae1687c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluate the quality of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d379257f1b42a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(k_eigenvectors, pseudo_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
