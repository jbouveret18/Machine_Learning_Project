{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1db54c87ef1cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initial Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4ddc06e7fedf8ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.568218900Z",
     "start_time": "2024-01-03T16:44:02.693234100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86669f7f9aa2933",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Loading the data\n",
    "For MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a603f956639a2628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.743102100Z",
     "start_time": "2024-01-03T16:44:02.727920500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "desired_length = 1000\n",
    "\n",
    "# Create indices for the subsampled dataset (randomly sampled)\n",
    "indices = list(range(len(mnist_trainset)))\n",
    "subset_indices = random.sample(indices, desired_length)\n",
    "\n",
    "# Create a Subset using the indices\n",
    "mnist_trainset = Subset(mnist_trainset, subset_indices)\n",
    "\n",
    "# Create a DataLoader for the subsampled dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ce8249d1c488c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Paper results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bea59dff45c49e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.805590900Z",
     "start_time": "2024-01-03T16:44:02.798607Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results obtained in the paper:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MNIST ACC</th>\n",
       "      <th>MNIST NMI</th>\n",
       "      <th>USPS ACC</th>\n",
       "      <th>USPS NMI</th>\n",
       "      <th>CIFAR-10 ACC</th>\n",
       "      <th>CIFAR-10 NMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-means</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Cluster</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep K-means</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSC No Flatten</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSC No Filter</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CSC No Voting</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CSC</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Method   MNIST ACC   MNIST NMI   USPS ACC   USPS NMI  CIFAR-10 ACC  \\\n",
       "0         K-means        0.58        0.49       0.48       0.42          0.14   \n",
       "1    Deep Cluster        0.86        0.83       0.67       0.69            NA   \n",
       "2    Deep K-means        0.84        0.80       0.76       0.78            NA   \n",
       "3  CSC No Flatten        0.85        0.79       0.83       0.78          0.12   \n",
       "4   CSC No Filter        0.83        0.76       0.84       0.79          0.14   \n",
       "5   CSC No Voting        0.82        0.77       0.82       0.76          0.14   \n",
       "6             CSC        0.86        0.81       0.83       0.79          0.15   \n",
       "\n",
       "   CIFAR-10 NMI  \n",
       "0          0.12  \n",
       "1            NA  \n",
       "2            NA  \n",
       "3          0.08  \n",
       "4          0.10  \n",
       "5          0.10  \n",
       "6          0.11  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'paper_results.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Results obtained in the paper:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521320c9",
   "metadata": {},
   "source": [
    "### A - 1) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "716e1287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:03.810607500Z",
     "start_time": "2024-01-03T16:44:02.830455200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data normalization with min-max method\n",
    "def min_max_scale_dataset(dataset, batch_size=64):\n",
    "    \n",
    "    min_pixel_value = float('inf')\n",
    "    max_pixel_value = float('-inf')\n",
    "\n",
    "    for images, _ in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "        min_pixel_value = min(min_pixel_value, images.min())\n",
    "        max_pixel_value = max(max_pixel_value, images.max())\n",
    "\n",
    "    \n",
    "    min_max_scaler = Normalize(min_pixel_value, max_pixel_value)\n",
    "    # Apply the min-max scaling to the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), min_max_scaler])\n",
    "    normalized_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    #select only 1000 datapoints\n",
    "    desired_length = 1000\n",
    "    indices = list(range(len(normalized_dataset)))\n",
    "    subset_indices = random.sample(indices, desired_length)\n",
    "    subset = Subset(normalized_dataset, subset_indices)\n",
    "\n",
    "    normalized_loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return normalized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d36cb56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.040955800Z",
     "start_time": "2024-01-03T16:44:02.862129300Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_train_loader = min_max_scale_dataset(mnist_trainset)\n",
    "mnist_test_loader = min_max_scale_dataset(mnist_testset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f7da262358c24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A - 2) Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48b0a",
   "metadata": {},
   "source": [
    "The autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "974bbb9f6e96ade2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.040955800Z",
     "start_time": "2024-01-03T16:44:06.024801Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Linear(16 * 14 * 14, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        flatten_x = x.view(x.size(0), -1)  # Flatten the output from the encoder\n",
    "        bottleneck_features = self.bottleneck(flatten_x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return bottleneck_features, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d38714f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.151947400Z",
     "start_time": "2024-01-03T16:44:06.040955800Z"
    }
   },
   "outputs": [],
   "source": [
    "input_channels_mnist = 1  # MNIST images are grayscale\n",
    "input_channels_cifar = 3  # CIFAR-10 images have 3 channels (RGB)\n",
    "\n",
    "mnist_autoencoder = ConvAutoencoder(input_channels_mnist)\n",
    "cifar_autoencoder = ConvAutoencoder(input_channels_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cd93bd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.168044900Z",
     "start_time": "2024-01-03T16:44:06.122035400Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_mnist = torch.optim.Adam(mnist_autoencoder.parameters(), lr=0.001)\n",
    "optimizer_cifar = torch.optim.Adam(cifar_autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4ced",
   "metadata": {},
   "source": [
    "The loop for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1369bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:06.199309300Z",
     "start_time": "2024-01-03T16:44:06.136044900Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 10\n",
    "def training_loop (model, loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        \n",
    "        for data in loader:\n",
    "            img, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCH}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c5afdbae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:11.034185200Z",
     "start_time": "2024-01-03T16:44:06.153034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2540\n",
      "Epoch [2/10], Loss: 0.2367\n",
      "Epoch [3/10], Loss: 0.2174\n",
      "Epoch [4/10], Loss: 0.1961\n",
      "Epoch [5/10], Loss: 0.1737\n",
      "Epoch [6/10], Loss: 0.1511\n",
      "Epoch [7/10], Loss: 0.1312\n",
      "Epoch [8/10], Loss: 0.1126\n",
      "Epoch [9/10], Loss: 0.0960\n",
      "Epoch [10/10], Loss: 0.0816\n"
     ]
    }
   ],
   "source": [
    "mnist_autoencoder = training_loop(mnist_autoencoder,mnist_train_loader,optimizer_mnist)\n",
    "#cifar_autoencoder = training_loop(cifar_autoencoder,cifar_train_loader,optimizer_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c99cc1",
   "metadata": {},
   "source": [
    "The evaluation loop to extract 500 features from the bottleneck layer for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6b57c362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:11.063486700Z",
     "start_time": "2024-01-03T16:44:11.046272700Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation (model, loader):\n",
    "    '''\n",
    "    output shape : bottelneck_feature (nb img,500)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    bottleneck_features_array = []\n",
    "    outputs_array = []\n",
    "    total_loss = 0\n",
    " \n",
    "    for data in loader:\n",
    "        img, _ = data\n",
    "       \n",
    "        bottleneck_features, output = model(img)\n",
    "\n",
    "        for features in bottleneck_features:\n",
    "            bottleneck_features_array.append(features.detach().numpy())\n",
    "        outputs_array.append(output.detach().numpy())\n",
    "\n",
    "        loss = criterion(output, img)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    print(\"Evaluation loss : \",average_loss)\n",
    "    \n",
    "    return np.array(bottleneck_features_array), outputs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e7f7005b5d1e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### B - Best features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140d2d8",
   "metadata": {},
   "source": [
    "Do the Non negativ Matrix Factorization (NMF) to estimate a decomposition W*H from V \\\n",
    "Calcul the error rate E from W*H+E = V \\\n",
    "Sort feature by error rate and keep the 50% best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4313031f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:11.093991Z",
     "start_time": "2024-01-03T16:44:11.050560300Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_filtering_nmf(features, k=1, removal_percentage=50):\n",
    "    '''\n",
    "    output shape : (nb img,250)\n",
    "    '''\n",
    "\n",
    "    #it doesn't work because of negative value so we remove the negative here\n",
    "    min_value = np.min(features)\n",
    "    features_shifted = features - min_value + 1e-10 \n",
    "\n",
    "    # NMF : find the decomposition V=W*H\n",
    "    nmf_model = NMF(n_components=k, init='random', random_state=None)\n",
    "    nmf_features = nmf_model.fit_transform(features_shifted)\n",
    "\n",
    "    #reconstruc W*H to evaluate with V to find E\n",
    "    reconstructed_features = np.dot(nmf_features, nmf_model.components_)\n",
    "    reconstruction_error = np.abs(features - reconstructed_features)\n",
    "\n",
    "    # Sort features by their E\n",
    "    sorted_indices = np.argsort(reconstruction_error, axis=1)\n",
    "    num_features_to_keep = int((100 - removal_percentage) / 100 * features.shape[1])\n",
    "    selected_features = features[np.arange(features.shape[0])[:, None], sorted_indices[:, :num_features_to_keep]]\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(selected_features)\n",
    "\n",
    "    return np.array(standardized_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94783182",
   "metadata": {},
   "source": [
    "### Loop over A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8efd9",
   "metadata": {},
   "source": [
    "do a loop over steps A and B  to increase number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "27c69341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.005586600Z",
     "start_time": "2024-01-03T16:44:11.063486700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss :  0.08111747447401285\n",
      "Evaluation loss :  0.08110800059512258\n",
      "Evaluation loss :  0.08110654586926103\n",
      "Evaluation loss :  0.08110963786020875\n",
      "Evaluation loss :  0.08112103212624788\n",
      "Evaluation loss :  0.08110666135326028\n",
      "Evaluation loss :  0.08111035078763962\n",
      "Evaluation loss :  0.0811111694201827\n",
      "Evaluation loss :  0.08110742270946503\n",
      "Evaluation loss :  0.08110571559518576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.5090107 ,  0.23449323,  0.08312765, ...,  1.6877185 ,\n",
       "         0.5099983 , -1.462617  ],\n",
       "       [-2.2245822 , -0.59771365, -1.9275753 , ...,  1.442345  ,\n",
       "         0.05739015,  0.79237   ],\n",
       "       [ 0.05494425,  0.50627655, -1.3250059 , ..., -0.29777125,\n",
       "         0.00958227,  1.9062787 ],\n",
       "       ...,\n",
       "       [-2.855108  , -0.34227055, -1.0463021 , ...,  1.2188302 ,\n",
       "        -1.3655035 , -1.5777268 ],\n",
       "       [ 0.63287157, -1.5598602 ,  0.99546474, ...,  0.08805925,\n",
       "        -2.3163583 ,  1.2161319 ],\n",
       "       [-1.2884805 , -0.4293022 , -2.388851  , ..., -0.9516373 ,\n",
       "         0.13769329,  1.4528776 ]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix = None\n",
    "\n",
    "for i in range(10):\n",
    "    features,_ = evaluation(mnist_autoencoder,mnist_test_loader)\n",
    "    sorted_features = feature_filtering_nmf(features)\n",
    "    \n",
    "    if i == 0 :\n",
    "        features_matrix = sorted_features\n",
    "    else :\n",
    "        features_matrix = np.concatenate((features_matrix,sorted_features),axis=1)\n",
    "        \n",
    "features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74355b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### C - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbaec17cae500b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03a52553f222d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "36a3d2d118485b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.044849200Z",
     "start_time": "2024-01-03T16:44:16.005586600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b547df21df315",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "38c2db1ed18a3a73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.047360Z",
     "start_time": "2024-01-03T16:44:16.023987500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()  # Adding a sigmoid activation function\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))  # Applying sigmoid activation\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3dd4e1990265109a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.078781900Z",
     "start_time": "2024-01-03T16:44:16.032834100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SecondDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(SecondDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 2 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5be6eff65c6981e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.078781900Z",
     "start_time": "2024-01-03T16:44:16.047360Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ThirdDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(ThirdDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.leaky_relu(self.fc1(z))\n",
    "        z = self.leaky_relu(self.fc2(z))\n",
    "        z = self.leaky_relu(self.fc3(z))\n",
    "        reconstruction = self.tanh(self.fc4(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc330de69b2fabd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c200a1da49e97f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create instances of each encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e64a6baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.139203100Z",
     "start_time": "2024-01-03T16:44:16.068289200Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2500\n",
    "hidden_dim = 500\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "17adb35de0caf903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.214978900Z",
     "start_time": "2024-01-03T16:44:16.086696500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Decoder(\n",
       "   (fc2): Linear(in_features=100, out_features=500, bias=True)\n",
       "   (fc3): Linear(in_features=500, out_features=2500, bias=True)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " SecondDecoder(\n",
       "   (fc1): Linear(in_features=100, out_features=1000, bias=True)\n",
       "   (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "   (fc3): Linear(in_features=1000, out_features=2500, bias=True)\n",
       "   (relu): ReLU()\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " ThirdDecoder(\n",
       "   (fc1): Linear(in_features=100, out_features=2000, bias=True)\n",
       "   (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "   (fc3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "   (fc4): Linear(in_features=500, out_features=2500, bias=True)\n",
       "   (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       "   (tanh): Tanh()\n",
       " )]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "secondDecoder = SecondDecoder(latent_dim, hidden_dim,input_dim)\n",
    "thirdDecoder = ThirdDecoder(latent_dim, hidden_dim,input_dim)\n",
    "\n",
    "# Put all encoders into a list\n",
    "all_decoders = [decoder, secondDecoder, thirdDecoder]\n",
    "all_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5cef2933ff119699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.240791Z",
     "start_time": "2024-01-03T16:44:16.219191400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, all_decoders):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, hidden_dim, input_dim)\n",
    "        self.decoders = nn.ModuleList(all_decoders)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        reconstructions = []\n",
    "        for decoder in self.decoders:\n",
    "            reconstruction = decoder(z)\n",
    "            reconstructions.append(reconstruction)\n",
    "\n",
    "        return reconstructions, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc50f7f645c43b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f3047d0a09085996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.234938200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (fc1): Linear(in_features=2500, out_features=500, bias=True)\n",
       "    (fc_mu): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (fc_logvar): Linear(in_features=500, out_features=100, bias=True)\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (fc2): Linear(in_features=100, out_features=500, bias=True)\n",
       "      (fc3): Linear(in_features=500, out_features=2500, bias=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): SecondDecoder(\n",
       "      (fc1): Linear(in_features=100, out_features=1000, bias=True)\n",
       "      (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "      (fc3): Linear(in_features=1000, out_features=2500, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): ThirdDecoder(\n",
       "      (fc1): Linear(in_features=100, out_features=2000, bias=True)\n",
       "      (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "      (fc3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "      (fc4): Linear(in_features=500, out_features=2500, bias=True)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(all_decoders)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629b79e113ec555",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8bf66b533d875023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.273579900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the data to be in the [0, 1] range\n",
    "max_val = features_matrix.max()\n",
    "min_val = features_matrix.min()\n",
    "normalized_features_matrix = (features_matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "features_tensor = torch.Tensor(normalized_features_matrix)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(features_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Dimension of data_loader : [938,1,64,2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1328f5e9312a82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Define the loss function (using the reconstruction loss and the KL divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c4cf9a2bde7e4a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.310310800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #print(f\"recon_x shape: {recon_x.shape}, x shape: {x.shape}\")\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee539886f49a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "98a8f9600fb1c972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:16.353247900Z",
     "start_time": "2024-01-03T16:44:16.323123600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ded01161eb1ddb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e0bce3aa039323c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.324616Z",
     "start_time": "2024-01-03T16:44:16.336977300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1743.2358\n",
      "Epoch [2/10], Loss: 1736.9198\n",
      "Epoch [3/10], Loss: 1734.8615\n",
      "Epoch [4/10], Loss: 1733.8452\n",
      "Epoch [5/10], Loss: 1733.0767\n",
      "Epoch [6/10], Loss: 1732.5852\n",
      "Epoch [7/10], Loss: 1732.2618\n",
      "Epoch [8/10], Loss: 1731.9439\n",
      "Epoch [9/10], Loss: 1731.6566\n",
      "Epoch [10/10], Loss: 1731.5085\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructions, mu, logvar = model(batch)\n",
    "\n",
    "        # Calculate the loss using your custom loss function\n",
    "        loss = loss_function(reconstructions[0], batch, mu, logvar)  # Assuming using the first decoder for loss calculation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "398a62c811edb4a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.684676200Z",
     "start_time": "2024-01-03T16:44:24.324616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "final_representations = []\n",
    "\n",
    "\n",
    "for batch in data_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs, _, _ = model(batch)\n",
    "\n",
    "    representations = torch.stack(outputs, dim=1)\n",
    "\n",
    "    final_representations.append(representations)\n",
    "\n",
    "\n",
    "final_representations = torch.cat(final_representations, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bd7fef233b5e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### D- Basic Subspace Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7da16f4e8963c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Compute the similarity matrix using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b221ad65c8da1d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:24.830289300Z",
     "start_time": "2024-01-03T16:44:24.696642800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "(3, 1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "similarity_list = []\n",
    "for i in range(3):\n",
    "    mat = final_representations[:,i,:]\n",
    "    similarity_matrix = cosine_similarity(mat)\n",
    "    print(similarity_matrix.shape)\n",
    "    similarity_list.append(similarity_matrix)\n",
    "\n",
    "similarity_list = np.array(similarity_list)\n",
    "print(similarity_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b40c915eff8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Generate the graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e61eb5403d21469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:26.926056700Z",
     "start_time": "2024-01-03T16:44:24.831257600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000, 1000)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_list = []\n",
    "for similarity_matrix in similarity_list :\n",
    "    degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n",
    "    laplacian_matrix = degree_matrix - similarity_matrix\n",
    "    normalized_laplacian = np.dot(np.dot(np.sqrt(np.linalg.inv(degree_matrix)), laplacian_matrix), np.sqrt(np.linalg.inv(degree_matrix)))\n",
    "    laplacian_list.append(normalized_laplacian)\n",
    "\n",
    "laplacian_list = np.array(laplacian_list)\n",
    "laplacian_list.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed266b3d289bd58",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Compute the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a91f79ac8670e2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:28.535538400Z",
     "start_time": "2024-01-03T16:44:26.924181Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 1000)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_eigen_vector_list = []\n",
    "for i in range(len(laplacian_list)):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian_list[i])\n",
    "    k = 10\n",
    "    list_k = []\n",
    "    for j in range(k): \n",
    "        k_eigenvectors = eigenvectors[:, j] / np.linalg.norm(eigenvectors[:, j])\n",
    "        list_k.append(k_eigenvectors)\n",
    "    k_eigen_vector_list.append(list_k)\n",
    "k_eigen_vector_list = np.array(k_eigen_vector_list)\n",
    "print(k_eigen_vector_list.shape)\n",
    "k_eigenvectors_matrix = np.reshape(k_eigen_vector_list,(30,1000))\n",
    "k_eigenvectors_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab9234001e7cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Perform K-Means clustering on selected eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5f3316c51061aa2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:30.706315Z",
     "start_time": "2024-01-03T16:44:28.532013900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum R ratio: 0.08098910003900528 achieved with 8 clusters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def kmean():\n",
    "    max_r_ratio = float('-inf')\n",
    "    best_r_ratio_clusters = None\n",
    "\n",
    "    for i in range(2, 30):  # Starting from 2 clusters as a minimum\n",
    "        spectral_model = SpectralClustering(n_clusters=i, affinity='nearest_neighbors')\n",
    "        pseudo_labels = spectral_model.fit_predict(k_eigenvectors_matrix)\n",
    "        # Calculate silhouette score as an example metric\n",
    "        silhouette = silhouette_score(k_eigenvectors_matrix, pseudo_labels)\n",
    "        if silhouette > max_r_ratio:\n",
    "            max_r_ratio = silhouette\n",
    "            best_r_ratio_clusters = i\n",
    "            \n",
    "\n",
    "    print(f\"Maximum R ratio: {max_r_ratio} achieved with {best_r_ratio_clusters} clusters\")\n",
    "    return i\n",
    "\n",
    "nb_clusters = kmean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038dca2",
   "metadata": {},
   "source": [
    "#### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "af174cdaccd6754f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T16:44:30.801394800Z",
     "start_time": "2024-01-03T16:44:30.706315Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025\n"
     ]
    }
   ],
   "source": [
    "spectral_model = SpectralClustering(n_clusters=nb_clusters, affinity='nearest_neighbors')\n",
    "pseudo_labels = spectral_model.fit_predict(features_matrix)\n",
    "\n",
    "#get all original labels\n",
    "all_targets =  []\n",
    "for _,targets in mnist_train_loader:\n",
    "     all_targets= np.concatenate((all_targets,targets.tolist()))\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "accuracy = accuracy_score(all_targets, pseudo_labels)\n",
    "print(accuracy)\n",
    " \n",
    "# VERY BAD xd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
