{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1db54c87ef1cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initial Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4ddc06e7fedf8ce9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:39:05.010088Z",
     "start_time": "2024-01-03T15:39:04.962452300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86669f7f9aa2933",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.***[Loading the data]***:\n",
    "For MNIST:\n",
    "In Python using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a603f956639a2628",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:52.993150700Z",
     "start_time": "2024-01-03T15:10:52.086174200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformations to apply to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "desired_length = 1000\n",
    "\n",
    "# Create indices for the subsampled dataset (randomly sampled)\n",
    "indices = list(range(len(mnist_trainset)))\n",
    "subset_indices = random.sample(indices, desired_length)\n",
    "\n",
    "# Create a Subset using the indices\n",
    "mnist_trainset = Subset(mnist_trainset, subset_indices)\n",
    "\n",
    "# Create a DataLoader for the subsampled dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77672e2c3020551d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For CIFAR-10:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ce8249d1c488c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Paper results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bea59dff45c49e93",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:53.087366900Z",
     "start_time": "2024-01-03T15:10:52.159630400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results obtained in the paper:\n"
     ]
    },
    {
     "data": {
      "text/plain": "           Method   MNIST ACC   MNIST NMI   USPS ACC   USPS NMI  CIFAR-10 ACC  \\\n0         K-means        0.58        0.49       0.48       0.42          0.14   \n1    Deep Cluster        0.86        0.83       0.67       0.69            NA   \n2    Deep K-means        0.84        0.80       0.76       0.78            NA   \n3  CSC No Flatten        0.85        0.79       0.83       0.78          0.12   \n4   CSC No Filter        0.83        0.76       0.84       0.79          0.14   \n5   CSC No Voting        0.82        0.77       0.82       0.76          0.14   \n6             CSC        0.86        0.81       0.83       0.79          0.15   \n\n   CIFAR-10 NMI  \n0          0.12  \n1            NA  \n2            NA  \n3          0.08  \n4          0.10  \n5          0.10  \n6          0.11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>MNIST ACC</th>\n      <th>MNIST NMI</th>\n      <th>USPS ACC</th>\n      <th>USPS NMI</th>\n      <th>CIFAR-10 ACC</th>\n      <th>CIFAR-10 NMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>K-means</td>\n      <td>0.58</td>\n      <td>0.49</td>\n      <td>0.48</td>\n      <td>0.42</td>\n      <td>0.14</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Deep Cluster</td>\n      <td>0.86</td>\n      <td>0.83</td>\n      <td>0.67</td>\n      <td>0.69</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Deep K-means</td>\n      <td>0.84</td>\n      <td>0.80</td>\n      <td>0.76</td>\n      <td>0.78</td>\n      <td>NA</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CSC No Flatten</td>\n      <td>0.85</td>\n      <td>0.79</td>\n      <td>0.83</td>\n      <td>0.78</td>\n      <td>0.12</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CSC No Filter</td>\n      <td>0.83</td>\n      <td>0.76</td>\n      <td>0.84</td>\n      <td>0.79</td>\n      <td>0.14</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CSC No Voting</td>\n      <td>0.82</td>\n      <td>0.77</td>\n      <td>0.82</td>\n      <td>0.76</td>\n      <td>0.14</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CSC</td>\n      <td>0.86</td>\n      <td>0.81</td>\n      <td>0.83</td>\n      <td>0.79</td>\n      <td>0.15</td>\n      <td>0.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'paper_results.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Results obtained in the paper:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521320c9",
   "metadata": {},
   "source": [
    "### A - 1) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "716e1287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:53.102991400Z",
     "start_time": "2024-01-03T15:10:52.244702100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data normalization with min-max method\n",
    "def min_max_scale_dataset(dataset, batch_size=64):\n",
    "    \n",
    "    min_pixel_value = float('inf')\n",
    "    max_pixel_value = float('-inf')\n",
    "\n",
    "    for images, _ in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "        min_pixel_value = min(min_pixel_value, images.min())\n",
    "        max_pixel_value = max(max_pixel_value, images.max())\n",
    "\n",
    "    \n",
    "    min_max_scaler = Normalize(min_pixel_value, max_pixel_value)\n",
    "    # Apply the min-max scaling to the dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor(), min_max_scaler])\n",
    "    normalized_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    #select only 1000 datapoints\n",
    "    desired_length = 1000\n",
    "    indices = list(range(len(normalized_dataset)))\n",
    "    subset_indices = random.sample(indices, desired_length)\n",
    "    subset = Subset(normalized_dataset, subset_indices)\n",
    "\n",
    "    normalized_loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return normalized_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d294bff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:53.102991400Z",
     "start_time": "2024-01-03T15:10:52.256255400Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d36cb56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:55.022146900Z",
     "start_time": "2024-01-03T15:10:52.269103600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "mnist_train_loader = min_max_scale_dataset(mnist_trainset)\n",
    "mnist_test_loader = min_max_scale_dataset(mnist_testset)\n",
    "\n",
    "print(len(mnist_train_loader))\n",
    "print(len(mnist_test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f7da262358c24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A - 2) Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e48b0a",
   "metadata": {},
   "source": [
    "The autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "974bbb9f6e96ade2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:55.027658Z",
     "start_time": "2024-01-03T15:10:55.022146900Z"
    }
   },
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Linear(16 * 14 * 14, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        flatten_x = x.view(x.size(0), -1)  # Flatten the output from the encoder\n",
    "        bottleneck_features = self.bottleneck(flatten_x)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return bottleneck_features, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d38714f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:55.203710Z",
     "start_time": "2024-01-03T15:10:55.027658Z"
    }
   },
   "outputs": [],
   "source": [
    "input_channels_mnist = 1  # MNIST images are grayscale\n",
    "input_channels_cifar = 3  # CIFAR-10 images have 3 channels (RGB)\n",
    "\n",
    "mnist_autoencoder = ConvAutoencoder(input_channels_mnist)\n",
    "cifar_autoencoder = ConvAutoencoder(input_channels_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd93bd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:55.203710Z",
     "start_time": "2024-01-03T15:10:55.101170600Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_mnist = torch.optim.Adam(mnist_autoencoder.parameters(), lr=0.001)\n",
    "optimizer_cifar = torch.optim.Adam(cifar_autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4ced",
   "metadata": {},
   "source": [
    "The loop for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1369bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:55.203710Z",
     "start_time": "2024-01-03T15:10:55.108806400Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 10\n",
    "def training_loop (model, loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        \n",
    "        for data in loader:\n",
    "            img, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCH}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5afdbae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:59.725801300Z",
     "start_time": "2024-01-03T15:10:55.124020700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1822\n",
      "Epoch [2/10], Loss: 0.1699\n",
      "Epoch [3/10], Loss: 0.1593\n",
      "Epoch [4/10], Loss: 0.1475\n",
      "Epoch [5/10], Loss: 0.1341\n",
      "Epoch [6/10], Loss: 0.1202\n",
      "Epoch [7/10], Loss: 0.1069\n",
      "Epoch [8/10], Loss: 0.0937\n",
      "Epoch [9/10], Loss: 0.0814\n",
      "Epoch [10/10], Loss: 0.0696\n"
     ]
    }
   ],
   "source": [
    "mnist_autoencoder = training_loop(mnist_autoencoder,mnist_train_loader,optimizer_mnist)\n",
    "#cifar_autoencoder = training_loop(cifar_autoencoder,cifar_train_loader,optimizer_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c99cc1",
   "metadata": {},
   "source": [
    "# The evaluation loop to extract 500 features from the bottleneck layer for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b57c362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:59.769836500Z",
     "start_time": "2024-01-03T15:10:59.716800500Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation (model, loader):\n",
    "    '''\n",
    "    output shape : bottelneck_feature (nb img,500)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    bottleneck_features_array = []\n",
    "    outputs_array = []\n",
    "    total_loss = 0\n",
    " \n",
    "    for data in loader:\n",
    "        img, _ = data\n",
    "       \n",
    "        bottleneck_features, output = model(img)\n",
    "\n",
    "        for features in bottleneck_features:\n",
    "            bottleneck_features_array.append(features.detach().numpy())\n",
    "        outputs_array.append(output.detach().numpy())\n",
    "\n",
    "        loss = criterion(output, img)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    print(\"Evaluation loss : \",average_loss)\n",
    "    \n",
    "    return np.array(bottleneck_features_array), outputs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e7f7005b5d1e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### B - Best features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140d2d8",
   "metadata": {},
   "source": [
    "Do the Non negativ Matrix Factorization (NMF) to estimate a decomposition W*H from V \\\n",
    "Calcul the error rate E from W*H+E = V \\\n",
    "Sort feature by error rate and keep the 50% best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4313031f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:10:59.769836500Z",
     "start_time": "2024-01-03T15:10:59.725801300Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_filtering_nmf(features, k=1, removal_percentage=50):\n",
    "    '''\n",
    "    output shape : (nb img,250)\n",
    "    '''\n",
    "\n",
    "    #it doesn't work because of negative value so we remove the negative here\n",
    "    min_value = np.min(features)\n",
    "    features_shifted = features - min_value + 1e-10 \n",
    "\n",
    "    # NMF : find the decomposition V=W*H\n",
    "    nmf_model = NMF(n_components=k, init='random', random_state=None)\n",
    "    nmf_features = nmf_model.fit_transform(features_shifted)\n",
    "\n",
    "    #reconstruc W*H to evaluate with V to find E\n",
    "    reconstructed_features = np.dot(nmf_features, nmf_model.components_)\n",
    "    reconstruction_error = np.abs(features - reconstructed_features)\n",
    "\n",
    "    # Sort features by their E\n",
    "    sorted_indices = np.argsort(reconstruction_error, axis=1)\n",
    "    num_features_to_keep = int((100 - removal_percentage) / 100 * features.shape[1])\n",
    "    selected_features = features[np.arange(features.shape[0])[:, None], sorted_indices[:, :num_features_to_keep]]\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(selected_features)\n",
    "\n",
    "    return np.array(standardized_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94783182",
   "metadata": {},
   "source": [
    "### Loop over A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8efd9",
   "metadata": {},
   "source": [
    "do a loop over steps A and B  to increase number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27c69341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.744106400Z",
     "start_time": "2024-01-03T15:10:59.738049800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss :  0.06898373551666737\n",
      "Evaluation loss :  0.06898058205842972\n",
      "Evaluation loss :  0.06898874184116721\n",
      "Evaluation loss :  0.06899185525253415\n",
      "Evaluation loss :  0.06897411122918129\n",
      "Evaluation loss :  0.0689823729917407\n",
      "Evaluation loss :  0.06897970009595156\n",
      "Evaluation loss :  0.06897666538134217\n",
      "Evaluation loss :  0.06897652382031083\n",
      "Evaluation loss :  0.0689868819899857\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.39192927, -0.43621045, -0.649946  , ..., -0.42949972,\n         0.12741743, -0.8781272 ],\n       [-0.6529905 ,  0.02645692, -0.75508296, ...,  1.0598505 ,\n         0.6478148 ,  1.1284541 ],\n       [ 0.02917459,  0.4808188 , -1.131926  , ...,  0.2801357 ,\n        -0.23586988, -0.16097274],\n       ...,\n       [-1.3182851 ,  0.24369086,  0.4573335 , ..., -1.1294991 ,\n         0.9824148 , -0.16075851],\n       [ 1.7241327 ,  0.90347856,  0.6246403 , ..., -0.48235545,\n         0.22270188, -2.204485  ],\n       [ 0.2460847 , -0.42467207,  0.34002817, ...,  2.4163647 ,\n        -0.69298244, -0.8502055 ]], dtype=float32)"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_matrix = None\n",
    "\n",
    "for i in range(10):\n",
    "    features,_ = evaluation(mnist_autoencoder,mnist_test_loader)\n",
    "    sorted_features = feature_filtering_nmf(features)\n",
    "    \n",
    "    if i == 0 :\n",
    "        features_matrix = sorted_features\n",
    "    else :\n",
    "        features_matrix = np.concatenate((features_matrix,sorted_features),axis=1)\n",
    "        \n",
    "features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74355b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### C - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbaec17cae500b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. # Define the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03a52553f222d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36a3d2d118485b21",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.758615500Z",
     "start_time": "2024-01-03T15:11:04.735928Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b547df21df315",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38c2db1ed18a3a73",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.831748700Z",
     "start_time": "2024-01-03T15:11:04.744106400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()  # Adding a sigmoid activation function\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))  # Applying sigmoid activation\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3dd4e1990265109a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.831748700Z",
     "start_time": "2024-01-03T15:11:04.768778900Z"
    }
   },
   "outputs": [],
   "source": [
    "class SecondDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(SecondDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 2 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        reconstruction = self.sigmoid(self.fc3(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5be6eff65c6981e1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.831748700Z",
     "start_time": "2024-01-03T15:11:04.787031800Z"
    }
   },
   "outputs": [],
   "source": [
    "class ThirdDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim):\n",
    "        super(ThirdDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.leaky_relu(self.fc1(z))\n",
    "        z = self.leaky_relu(self.fc2(z))\n",
    "        z = self.leaky_relu(self.fc3(z))\n",
    "        reconstruction = self.tanh(self.fc4(z))\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc330de69b2fabd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c200a1da49e97f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create instances of each encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e64a6baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.831748700Z",
     "start_time": "2024-01-03T15:11:04.803602400Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2500\n",
    "hidden_dim = 500\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "17adb35de0caf903",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:04.979574500Z",
     "start_time": "2024-01-03T15:11:04.816111800Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "secondDecoder = SecondDecoder(latent_dim, hidden_dim,input_dim)\n",
    "thirdDecoder = ThirdDecoder(latent_dim, hidden_dim,input_dim)\n",
    "\n",
    "\n",
    "# Put all encoders into a list\n",
    "all_decoders = [decoder, secondDecoder, thirdDecoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5cef2933ff119699",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:05.004757Z",
     "start_time": "2024-01-03T15:11:04.937329200Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, all_decoders):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, hidden_dim, input_dim)\n",
    "        self.decoders = nn.ModuleList(all_decoders)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        reconstructions = []\n",
    "        for decoder in self.decoders:\n",
    "            reconstruction = decoder(z)\n",
    "            reconstructions.append(reconstruction)\n",
    "\n",
    "        return reconstructions, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc50f7f645c43b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f3047d0a09085996",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:05.005268700Z",
     "start_time": "2024-01-03T15:11:04.948800900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VAE(\n  (encoder): Encoder(\n    (fc1): Linear(in_features=2500, out_features=500, bias=True)\n    (fc_mu): Linear(in_features=500, out_features=100, bias=True)\n    (fc_logvar): Linear(in_features=500, out_features=100, bias=True)\n  )\n  (decoders): ModuleList(\n    (0): Decoder(\n      (fc2): Linear(in_features=100, out_features=500, bias=True)\n      (fc3): Linear(in_features=500, out_features=2500, bias=True)\n      (sigmoid): Sigmoid()\n    )\n    (1): SecondDecoder(\n      (fc1): Linear(in_features=100, out_features=1000, bias=True)\n      (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n      (fc3): Linear(in_features=1000, out_features=2500, bias=True)\n      (relu): ReLU()\n      (sigmoid): Sigmoid()\n    )\n    (2): ThirdDecoder(\n      (fc1): Linear(in_features=100, out_features=2000, bias=True)\n      (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n      (fc3): Linear(in_features=1000, out_features=500, bias=True)\n      (fc4): Linear(in_features=500, out_features=2500, bias=True)\n      (leaky_relu): LeakyReLU(negative_slope=0.01)\n      (tanh): Tanh()\n    )\n  )\n)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(all_decoders)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629b79e113ec555",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. # Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bf66b533d875023",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:05.100597200Z",
     "start_time": "2024-01-03T15:11:04.983466800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data to be in the [0, 1] range\n",
    "max_val = features_matrix.max()\n",
    "min_val = features_matrix.min()\n",
    "normalized_features_matrix = (features_matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "features_tensor = torch.Tensor(normalized_features_matrix)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(features_tensor, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db7c9d",
   "metadata": {},
   "source": [
    "Dimension of data_loader : [938,1,64,2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1328f5e9312a82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. # Define the loss function (using the reconstruction loss and the KL divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4cf9a2bde7e4a33",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:05.139263100Z",
     "start_time": "2024-01-03T15:11:05.008200500Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    #print(f\"recon_x shape: {recon_x.shape}, x shape: {x.shape}\")\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee539886f49a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. # Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "98a8f9600fb1c972",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:05.139263100Z",
     "start_time": "2024-01-03T15:11:05.027888500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ded01161eb1ddb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. # Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fb843fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:05.139263100Z",
     "start_time": "2024-01-03T15:11:05.045896200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print (len(data_loader))\n",
    "for a,b in enumerate(data_loader):\n",
    "    print(type(b))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e0bce3aa039323c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:12.622577900Z",
     "start_time": "2024-01-03T15:11:05.065952100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1743.1046\n",
      "Epoch [2/10], Loss: 1736.6279\n",
      "Epoch [3/10], Loss: 1734.8132\n",
      "Epoch [4/10], Loss: 1733.6849\n",
      "Epoch [5/10], Loss: 1732.9595\n",
      "Epoch [6/10], Loss: 1732.5500\n",
      "Epoch [7/10], Loss: 1732.2896\n",
      "Epoch [8/10], Loss: 1731.9761\n",
      "Epoch [9/10], Loss: 1731.7153\n",
      "Epoch [10/10], Loss: 1731.5685\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructions, mu, logvar = model(batch)\n",
    "\n",
    "        # Calculate the loss using your custom loss function\n",
    "        loss = loss_function(reconstructions[0], batch, mu, logvar)  # Assuming using the first decoder for loss calculation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "398a62c811edb4a0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:12.938473400Z",
     "start_time": "2024-01-03T15:11:12.618469900Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "final_representations = []\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch in data_loader:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs, _, _ = model(batch)\n",
    "    \n",
    "    # Stack the representations along a new dimension (dim=1)\n",
    "    representations = torch.stack(outputs, dim=1)\n",
    "\n",
    "    # Append the representations to the final list\n",
    "    final_representations.append(representations)\n",
    "\n",
    "# Concatenate the representations along the batch dimension (dim=0)\n",
    "final_representations = torch.cat(final_representations, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c69090de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:12.967009500Z",
     "start_time": "2024-01-03T15:11:12.945473Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "897bd7fef233b5e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### D- Basic Subspace Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7da16f4e8963c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute the similarity matrix using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b221ad65c8da1d81",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:13.099893700Z",
     "start_time": "2024-01-03T15:11:12.960602900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "(3, 1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "#J'ai pas assez de ram sur mon pc mdr !!!! Ducoup je fais un PCA mais si ona le pc pour sa devrai fonctioner. PS: IL faut au loin 14GB de ram mdr donc un pc qui a 32GB de ram pour le tourner.\n",
    "\n",
    "similarity_list = []\n",
    "for i in range(3):\n",
    "    mat = final_representations[:,i,:]\n",
    "    similarity_matrix = cosine_similarity(mat)\n",
    "    print(similarity_matrix.shape)\n",
    "    similarity_list.append(similarity_matrix)\n",
    "\n",
    "similarity_list = np.array(similarity_list)\n",
    "print(similarity_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b40c915eff8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate the graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e61eb5403d21469",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:11:13.952651100Z",
     "start_time": "2024-01-03T15:11:13.084257200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1000, 1000)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_list = []\n",
    "for similarity_matrix in similarity_list :\n",
    "    degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n",
    "    laplacian_matrix = degree_matrix - similarity_matrix\n",
    "    normalized_laplacian = np.dot(np.dot(np.sqrt(np.linalg.inv(degree_matrix)), laplacian_matrix), np.sqrt(np.linalg.inv(degree_matrix)))\n",
    "    laplacian_list.append(normalized_laplacian)\n",
    "\n",
    "laplacian_list = np.array(laplacian_list)\n",
    "laplacian_list.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed266b3d289bd58",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a91f79ac8670e2fc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:32:57.462038300Z",
     "start_time": "2024-01-03T15:32:55.053660700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(30, 1000)"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_eigen_vector_list = []\n",
    "for i in range(len(laplacian_list)):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian_list[i])\n",
    "    k = 10\n",
    "    list_k = []\n",
    "    for j in range(k): \n",
    "        k_eigenvectors = eigenvectors[:, j] / np.linalg.norm(eigenvectors[:, j])\n",
    "        list_k.append(k_eigenvectors)\n",
    "    k_eigen_vector_list.append(list_k)\n",
    "k_eigen_vector_list = np.array(k_eigen_vector_list)\n",
    "print(k_eigen_vector_list.shape)\n",
    "k_eigenvectors_matrix = np.reshape(k_eigen_vector_list,(30,1000))\n",
    "k_eigenvectors_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab9234001e7cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Perform K-Means clustering on selected eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e54ea6cc938de703"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f3316c51061aa2f"
  },
  {
   "cell_type": "markdown",
   "id": "3faa2549fae1687c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluate the quality of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "100d379257f1b42a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:33:24.019287800Z",
     "start_time": "2024-01-03T15:33:23.664815400Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 3.42180841e-02 -1.06676053e-02  9.76189785e-03 -1.73727854e-03\n -5.90789914e-02 -1.84368331e-03  3.56702916e-02  1.39581482e-03\n  2.46395729e-02 -1.65780019e-02 -4.85542305e-02 -1.14951218e-02\n  6.57675089e-03 -2.73920428e-02  6.08483218e-02 -2.47640144e-02\n -1.55086080e-02  1.41540263e-03  1.04892654e-02 -3.47902328e-02\n -8.29673856e-02 -2.93387882e-02 -2.69389860e-02 -2.63558645e-02\n  3.87416594e-02  3.10202297e-02  6.06928580e-02  3.73603106e-02\n -5.28072705e-04  2.15034303e-03 -2.85482295e-02  8.57486855e-03\n -5.60640134e-02  3.59046762e-03  2.30839793e-02  1.33509394e-02\n -2.87624132e-02  4.74889129e-02  2.04882771e-02  4.59634885e-02\n  3.72267514e-02 -2.33419612e-02 -1.51904682e-02  3.63747440e-02\n  3.53081040e-02 -2.84880828e-02  2.65299324e-02 -2.76744775e-02\n  1.18166037e-01  3.81570943e-02 -2.77147666e-02 -2.37664282e-02\n  3.26162018e-03 -3.29441987e-02 -3.31442505e-02 -4.99936473e-03\n -2.97550634e-02  3.54483575e-02 -8.44130944e-03 -1.20985066e-03\n  3.43254432e-02 -7.36955777e-02  8.51478148e-03  1.95030142e-02\n -2.93148886e-02  2.28037611e-02  9.14611295e-03  5.74062653e-02\n -2.17369571e-02  7.79461069e-03  4.22228388e-02 -4.90873791e-02\n  2.94212662e-02 -4.48452942e-02  2.38211849e-03  1.11772493e-03\n -4.72990125e-02  1.30282473e-02  4.11145389e-02  5.70873581e-02\n -3.51253450e-02 -1.71891209e-02  2.87371501e-03  1.70566663e-02\n -1.03336656e-02  1.27872257e-02 -3.39109376e-02 -4.38410863e-02\n  1.79735869e-02  2.72876257e-03  1.87757667e-02  1.76043231e-02\n  2.57115569e-02 -4.22211885e-02 -1.95116252e-02 -3.34180356e-03\n  1.42156649e-02 -4.49861865e-03  3.60903777e-02 -1.50980661e-02\n  9.95518733e-03 -1.88651979e-02  3.70264798e-02  2.84435097e-02\n -3.71557474e-02  1.24887014e-02  2.64255591e-02  1.85980983e-02\n  5.37219038e-03  1.32010868e-02 -4.39589769e-02 -6.58578351e-02\n  4.54393029e-03  4.12329622e-02  2.95821279e-02  4.47114147e-02\n  3.89102213e-02  1.15194498e-02 -1.59769394e-02 -3.34593132e-02\n -4.26630899e-02 -2.83304304e-02 -3.40004475e-03 -2.63704341e-02\n  1.89394075e-02  4.50417548e-02  1.14878334e-04 -3.05763283e-03\n -2.15384960e-02  6.25467524e-02  6.42040893e-02  6.82053342e-02\n  6.18970441e-03 -5.02194017e-02 -2.70929746e-02 -3.84965725e-02\n  2.29256712e-02 -2.15272754e-02 -2.38366816e-02  5.08411322e-03\n  1.62763428e-02 -1.42969405e-02 -8.22864622e-02  9.03911665e-02\n -5.43838479e-02  6.78424025e-03  4.08152724e-03 -3.57618108e-02\n  1.16520850e-02  3.72144133e-02 -2.52784118e-02  1.54823381e-02\n  4.59182151e-02  8.42993520e-03 -2.88772788e-02 -1.88142750e-02\n -2.46940460e-02  1.09711336e-02  5.24007156e-02  3.34873646e-02\n  2.41335901e-03 -1.69386156e-02 -5.88801131e-02  3.84556279e-02\n  1.62078813e-02 -3.50344777e-02  1.22299064e-02 -2.25793514e-02\n  2.71182284e-02 -2.47923210e-02  5.11056324e-03 -3.03877406e-02\n -2.06495193e-03  1.80751141e-02  2.51253671e-03 -1.55187929e-02\n -1.95212907e-03  6.47532474e-03 -8.81842244e-03  5.77054285e-02\n  1.45415841e-02  5.69251403e-02 -1.24494852e-02  6.21847901e-03\n  1.71543267e-02  3.93836647e-02  3.19507867e-02  3.84223871e-02\n  1.11458063e-01  1.30681684e-02  2.10324340e-02 -4.69334349e-02\n  3.73610221e-02  8.40351917e-03 -3.42733189e-02 -1.78929307e-02\n  1.18498737e-02 -5.48488982e-02  2.92214546e-02  5.03540859e-02\n -2.98171230e-02 -2.10969206e-02  2.86634602e-02 -2.23388672e-02\n  3.04447953e-02  5.31621203e-02 -4.94061857e-02 -2.44240966e-02\n -1.79937191e-03 -1.16658537e-03  1.92979500e-02  3.44720073e-02\n -1.80062726e-02  2.52881963e-02 -9.90009308e-03  3.24260583e-03\n  6.70040026e-02  1.73029322e-02  5.05280197e-02 -6.32765144e-03\n -5.56908287e-02 -9.06402338e-03 -5.64708151e-02  4.39422950e-02\n -4.43273559e-02  1.07680745e-02  4.25554737e-02  2.39583962e-02\n -4.10647988e-02  7.72780599e-03 -1.19811669e-03  1.97420809e-02\n  6.57571331e-02 -1.15894573e-02 -7.24647194e-02 -1.21329017e-02\n  1.37987155e-02  3.37797450e-04 -1.93428658e-02  4.10099374e-03\n  2.64818650e-02  3.95532623e-02 -2.85804626e-02  3.56067858e-07\n  5.77828512e-02  3.72863300e-02 -3.88110057e-02 -2.56148800e-02\n  6.25245692e-03  1.54023441e-02 -1.75614879e-02  3.79495546e-02\n  3.72547247e-02  1.25374841e-02  5.49723841e-02  7.60745956e-03\n  4.18580808e-02  5.13962731e-02  9.40688979e-03 -1.38529437e-03\n -9.18448131e-05  9.43257660e-03 -2.46128105e-02 -5.40512837e-02\n  1.66218001e-02  1.52099878e-02 -5.94822541e-02  1.25577915e-02\n  1.62012167e-02  3.69100943e-02 -2.00299267e-02 -3.58892642e-02\n  1.90172307e-02 -2.61022132e-02  2.47226208e-02  2.51422338e-02\n  1.51010202e-02  1.46767255e-02 -3.15585323e-02 -1.70351053e-03\n -2.82006785e-02 -1.57529935e-02  3.46174054e-02  1.45949135e-02\n  3.95030268e-02  3.25984396e-02  1.13294711e-02  5.15526673e-03\n  1.07160211e-02  1.17506329e-02  4.83299866e-02 -4.25337330e-02\n  2.54633743e-02  3.31799663e-03  2.67793629e-02 -2.53778626e-03\n -1.70394313e-02 -1.66932940e-02  3.14745083e-02  1.89020429e-02\n  1.98835377e-02 -5.37078865e-02 -4.79939505e-02 -3.15622762e-02\n  4.76628318e-02  1.93342920e-02 -5.85542731e-02 -1.44373309e-02\n  8.05494841e-03  2.92822942e-02 -3.12911645e-02 -1.18430583e-02\n -4.37977873e-02  3.99416424e-02 -3.41954604e-02  3.68238986e-02\n  4.05534469e-02 -1.96357127e-02  1.80017054e-02  5.31825498e-02\n  1.90081801e-02  3.64036486e-02  2.94481404e-02  1.87339075e-02\n  6.55458644e-02  8.58169794e-03  6.82209507e-02  1.83735078e-03\n  2.16941182e-02  6.40307665e-02  3.34082916e-03  5.47589064e-02\n  1.31489364e-02 -1.34269958e-02 -1.22118564e-02 -2.89521758e-02\n  5.90809025e-02 -1.80458706e-02 -2.40770523e-02  4.01210561e-02\n -1.27519397e-02  6.90522417e-02 -2.71833390e-02  3.77944000e-02\n  2.15107333e-02 -6.09009378e-02 -5.46891838e-02  3.62452567e-02\n -4.99556027e-02  1.02656530e-02 -1.93464011e-02  3.26444060e-02\n  1.27624786e-02 -5.99939143e-03 -4.07625698e-02 -4.01184568e-03\n  1.36186117e-02 -4.13850183e-03  6.54283911e-03 -4.33826558e-02\n -5.57372384e-02 -5.90690784e-03 -1.19445911e-02  3.61640267e-02\n  1.05092174e-03  9.94909406e-02 -7.50018377e-03  2.59704366e-02\n -3.35371145e-03 -8.01269896e-03 -1.98051892e-02 -2.82358266e-02\n  1.50786787e-02  3.95017117e-03 -2.70488914e-02 -1.64064914e-02\n  7.32043525e-03  1.03483703e-02 -2.79483590e-02 -2.79650111e-02\n  2.61370391e-02  2.32125334e-02  6.01386465e-02  2.99330205e-02\n  1.50361918e-02 -1.16783222e-02 -3.54350619e-02 -9.53890197e-03\n -1.60268135e-02 -1.15521718e-02 -2.31285058e-02  1.65254101e-02\n -2.69472990e-02  1.42562911e-02  6.51261630e-03  2.81214509e-02\n -7.57706910e-02 -5.81581965e-02 -2.81328312e-03  2.03065239e-02\n -3.30131152e-03 -2.19425815e-03  3.69098261e-02  4.87924032e-02\n  3.26316018e-04 -6.70623314e-03  3.01186548e-04  2.25246530e-02\n  3.73362214e-03 -1.15546677e-02 -2.80533824e-02 -1.31824920e-02\n  6.40157610e-02 -7.52573758e-02 -2.87917280e-03  5.17578935e-03\n -3.66258458e-03  1.22846961e-02 -1.52258035e-02  4.62259687e-02\n -8.28521978e-03 -1.04054874e-02 -6.39904812e-02 -3.88383009e-02\n  1.96102969e-02  1.06616747e-02  1.55055514e-02  1.76628623e-02\n  2.19985703e-03  3.44655998e-02 -2.43697055e-02  4.24890108e-02\n -1.04218740e-02 -1.18980138e-02 -8.90897680e-03 -3.88931446e-02\n  4.49360237e-02 -9.32201091e-03 -1.26756392e-02  6.11680932e-03\n -1.19516980e-02 -3.91737837e-03 -3.62362973e-02 -6.13267161e-02\n  2.80768946e-02  2.19069049e-02 -1.32054882e-02  7.55391875e-03\n  1.85139233e-03 -4.61443812e-02 -3.78352962e-02 -1.06795793e-02\n -1.21175218e-03  4.19750251e-03 -9.55233816e-03 -2.03875713e-02\n  5.89707447e-03 -2.27986667e-02 -7.60537805e-03 -2.96444949e-02\n -3.23305950e-02  4.08668444e-02  2.27842703e-02 -4.75452580e-02\n -8.15098581e-04  1.11472495e-02 -1.20180594e-02 -2.85449494e-02\n  2.65816282e-02  9.82705597e-03 -3.19459885e-02 -2.22500786e-02\n  5.23840599e-02 -1.41348587e-02  1.78180709e-02  2.44422965e-02\n  2.99739242e-02 -4.02688682e-02  1.25280360e-03 -9.15325657e-02\n  6.86597750e-02  6.60150312e-03  3.76362610e-03  9.66899749e-03\n  1.88873503e-02  5.52567281e-03  2.80663334e-02 -2.94073950e-03\n  2.64876150e-02 -4.97503253e-03 -1.48841403e-02 -1.61051024e-02\n  2.50087474e-02  1.09365042e-02  1.45816626e-02  3.24027725e-02\n -6.57506706e-03 -2.48535201e-02  2.91602500e-02 -5.69668319e-03\n -4.85454127e-02 -2.25796923e-03 -2.85186134e-02 -1.55459382e-02\n -2.35996637e-02  2.39815027e-03  3.05724815e-02  5.41846361e-03\n  3.58869396e-02  5.07669337e-03  1.39000593e-03  3.76666361e-03\n  3.72482166e-02 -1.59859117e-02  3.23143192e-02  1.62717197e-02\n  4.96610552e-02  1.37279416e-02 -4.75539416e-02 -2.35693902e-03\n  4.01398167e-02  3.82967177e-03 -1.46265039e-02  8.99556279e-03\n  1.47170911e-03  1.66722722e-02  2.58590020e-02  9.08292644e-03\n -1.36549177e-03  1.42582897e-02 -3.90860587e-02  2.30081864e-02\n  1.68408565e-02  8.12727399e-03  4.27600481e-02  1.64819416e-02\n  1.61629748e-02 -8.36795941e-03 -2.47453470e-02 -1.24778841e-02\n  4.35280502e-02  2.32250411e-02  9.70627181e-03 -2.17560660e-02\n  2.37768441e-02 -8.35516118e-03  7.45645911e-03  1.02210259e-02\n  2.37595383e-02 -1.09368637e-02 -2.33560405e-03 -6.46449951e-03\n -5.24342107e-03 -1.59829087e-03 -6.51947856e-02 -3.01125962e-02\n -2.41542105e-02 -3.88235897e-02 -2.25255713e-02 -4.23029345e-03\n  1.64969862e-02 -5.01862392e-02 -5.07756472e-02 -3.17684934e-02\n  3.06716599e-02  3.77647243e-02  7.25280270e-02 -6.11166330e-03\n -8.05715856e-04 -4.08594543e-03 -3.67439166e-02 -4.18000035e-02\n  3.90075631e-02  4.89491783e-02 -1.17698535e-02 -4.61592712e-02\n -7.25825354e-02 -2.79057026e-02 -3.07498313e-02 -1.29949432e-02\n  2.33776327e-02 -2.11613178e-02  1.13216536e-02  2.88581848e-03\n -1.47881145e-02  1.20132929e-02 -7.31711835e-02 -2.34729238e-02\n  1.42925279e-02  4.60184403e-02  3.01881433e-02 -1.22445989e-02\n -2.71292496e-02 -3.24998889e-03 -2.80264616e-02  2.76431795e-02\n  8.16543400e-03 -8.55214149e-03  8.08408111e-03  2.57462524e-02\n  1.78627297e-02  1.87428650e-02 -1.49848191e-02 -6.33278303e-03\n  2.49050539e-02  6.40120870e-03 -2.62037758e-02 -3.51277888e-02\n -2.71785408e-02 -8.95567797e-03 -5.31988330e-02 -3.33282016e-02\n  4.20386158e-02 -4.17411588e-02  2.58229170e-02  3.45692299e-02\n  7.26388115e-03  1.13006309e-03  1.41074881e-02  1.51159137e-03\n -8.18297081e-03  3.74371782e-02 -6.41784593e-02 -1.82465538e-02\n -3.48709412e-02 -5.13107330e-02 -2.71874182e-02  1.21279480e-02\n  1.54595459e-02  4.14679497e-02  5.33081554e-02 -2.36858567e-03\n  6.06442466e-02 -3.45230699e-02  2.72924043e-02 -2.47241522e-04\n -3.78012024e-02 -1.75626334e-02  3.59302089e-02  2.50857789e-02\n -1.15140760e-02 -1.55295040e-02 -2.89102248e-03 -2.17212643e-02\n  4.16025147e-02  6.49670139e-03 -1.41804656e-02 -7.76072405e-03\n -2.81620473e-02 -1.95729136e-02 -2.77689509e-02 -2.11002436e-02\n  3.57708856e-02 -2.80954186e-02  4.34520468e-02  2.54419558e-02\n  1.93968019e-03  1.74011383e-02 -4.24527079e-02 -4.87458184e-02\n -4.20696661e-02 -3.65938693e-02 -3.56470682e-02 -1.93592999e-02\n  2.07568239e-02  1.73984829e-03 -5.27157895e-02  5.53154983e-02\n -2.41011530e-02  4.03166339e-02 -8.87010526e-03  1.21322721e-02\n -2.68715043e-02  4.78688069e-02  1.29871489e-02  1.38725089e-02\n  2.99002733e-02  3.46896090e-02  8.47313262e-04  2.21407600e-03\n -6.64938837e-02 -8.84330831e-03 -3.09231374e-02 -6.03915018e-04\n  3.20547298e-02 -7.81039102e-03  2.87739746e-02  3.05583496e-02\n  3.16287540e-02  8.56130291e-03  1.61118787e-02 -3.24605815e-02\n  3.39283161e-02 -5.41956201e-02  1.02180773e-02  3.53976563e-02\n  1.10155968e-02  1.68368313e-03 -2.18614694e-02 -6.73281178e-02\n -6.45067403e-03  3.33111244e-03  5.93389757e-02 -1.04912920e-02\n  2.90363636e-02  1.40792551e-02  6.30254578e-03  3.58664058e-02\n -2.53124479e-02  2.25652326e-02 -5.77359111e-04 -2.92755221e-03\n  3.63229169e-03  3.90454605e-02  9.51006822e-03 -3.24643962e-02\n -1.34666869e-02 -3.45175527e-02 -2.06782725e-02 -1.19680148e-02\n -7.34838173e-02  1.04327267e-03 -2.27143411e-02 -5.50386403e-03\n  2.59204339e-02 -2.13630330e-02 -4.74938229e-02 -9.48234461e-03\n  7.44100511e-02  7.08385110e-02  6.32095477e-03  2.66469698e-02\n  3.38760717e-03 -3.90653964e-03  1.16159860e-03 -3.13857459e-02\n -9.03026480e-03  2.65456680e-02 -8.78712069e-03 -1.14415807e-03\n  1.17063522e-03 -1.92478839e-02  3.68883945e-02 -1.62639469e-02\n -4.19347845e-02  4.35140952e-02 -3.73110287e-02  1.23325270e-02\n  2.19130181e-02  2.56084371e-02  7.25280046e-02  2.91179121e-02\n  8.36846828e-02 -4.35659029e-02 -8.35609250e-03  4.58607404e-03\n -5.59556996e-03  2.92071961e-02  3.93319428e-02  3.55720185e-02\n  1.98808759e-02 -4.54112738e-02  1.48039786e-02 -8.02166201e-03\n  1.59264961e-03 -8.39544367e-03 -3.28469537e-02  2.47576684e-02\n -7.61143193e-02 -2.08254084e-02  1.62602682e-02 -3.04639358e-02\n -2.17193533e-02 -4.43004072e-03 -1.52567113e-02 -4.84847687e-02\n -1.17601827e-02 -6.35271370e-02  4.24793269e-03 -2.31463071e-02\n -3.50244896e-04  1.86845157e-02  3.22816782e-02 -3.97873893e-02\n  3.62577848e-02 -4.68123741e-02 -1.79854073e-02  1.38707738e-03\n -2.24105828e-02  2.02228073e-02 -5.49124181e-02 -7.16325045e-02\n  4.79527004e-02 -4.67304103e-02  4.06398997e-02 -3.20381075e-02\n  5.60447760e-03  2.57881987e-03 -1.82499718e-02  5.19588590e-03\n  3.32999462e-03  2.25398857e-02  4.96628843e-02 -2.00414062e-02\n -2.90098302e-02  1.59345306e-02  3.59970927e-02  9.10980976e-04\n -2.53395867e-02  9.97680426e-03 -1.40363816e-02 -1.36225969e-02\n -3.99951404e-03  1.35265971e-02  1.67716690e-03 -5.48022129e-02\n -1.25386287e-02 -1.07047446e-02  3.30658443e-02 -1.76461320e-02\n  2.14501042e-02  3.02644689e-02 -5.83577296e-03 -1.96602382e-02\n -5.12638241e-02  2.26401333e-02 -1.32676661e-02  4.28262353e-02\n  4.36481386e-02  9.91506130e-03 -8.45065154e-03  1.79862715e-02\n -2.76043061e-02 -2.49280930e-02 -2.93538496e-02 -3.83058265e-02\n -3.74020264e-02  8.24281422e-04 -4.81557380e-03  1.30771264e-03\n  9.49643552e-03 -5.19729033e-02  3.81641500e-02  3.11720483e-02\n -1.15101235e-02 -2.94274278e-02  4.05922905e-03 -2.26000734e-02\n -2.03646980e-02 -9.22777504e-02 -1.85722522e-02  3.37464958e-02\n -5.15015125e-02  8.89871642e-03 -7.36175478e-03  8.27680808e-03\n -2.28725560e-02  3.26822363e-02  1.26480311e-02  2.26432066e-02\n -3.88532020e-02 -3.69665883e-02  1.44297518e-02 -1.54920481e-03\n  2.33350229e-03 -2.54025850e-02  2.36255154e-02 -2.74214577e-02\n  1.85194947e-02  5.97900935e-02 -5.90597838e-02 -3.35657853e-03\n -4.43326496e-02  2.10121106e-02 -5.84871322e-02  4.50278297e-02\n  5.39775118e-02 -3.43936197e-02  2.50951177e-03  1.19626289e-02\n -8.08885135e-03  2.10621506e-02 -3.95104773e-02  3.03309802e-02\n  5.23507893e-02 -3.81095819e-02 -7.63536990e-02 -2.54498273e-02\n -2.41915062e-02  9.57219221e-04  1.70935895e-02 -7.26494240e-03\n -1.71846244e-02 -4.07149903e-02  1.44807203e-02 -3.40680592e-02\n -1.02731222e-02 -3.74081507e-02 -1.15459282e-02 -1.65656619e-02\n  5.19147180e-02 -5.08948006e-02  1.14877324e-03 -4.33650352e-02\n -2.82994658e-02 -3.09068039e-02  3.38300364e-03 -3.97522897e-02\n  3.37059572e-02 -3.29212993e-02 -1.82965286e-02 -8.00489113e-02\n -8.56158584e-02 -3.00997086e-02 -4.18349169e-02 -1.06652146e-02\n -1.41634513e-02  4.86187041e-02 -5.79449534e-02 -3.26083899e-02\n -2.69033690e-03  2.53589619e-02 -2.50752177e-02  2.97869090e-02\n  6.95786923e-02  2.30584992e-03 -1.33689465e-02  2.21960526e-02\n -2.12461632e-02 -6.95791095e-02 -4.12467942e-02  7.95270875e-02\n -6.32799342e-02  7.43849948e-03 -4.88964394e-02  5.50187891e-03\n -2.40201633e-02 -2.19667014e-02  5.45400241e-03  1.39101334e-02\n -2.81435028e-02  4.82508205e-02 -5.28882677e-03 -5.40093984e-03\n  1.67793632e-02 -1.00305472e-02  6.03833003e-04 -4.78425669e-03\n -1.53062753e-02  4.09782231e-02  2.98490282e-02  1.83052961e-02\n -4.79068607e-02 -2.59555597e-02 -2.86078430e-03 -1.04995100e-02\n -1.04182782e-02  1.88700687e-02  3.67380902e-02 -2.92639118e-02\n -4.41675335e-02  1.61481043e-03 -1.91482399e-02  1.47453621e-02\n  2.52718665e-02  1.11932578e-02  6.92534400e-03  4.01024930e-02\n  2.76672114e-02 -3.71680893e-02  2.32658423e-02  4.40677702e-02\n -1.45099568e-03  2.59655938e-02  3.54468375e-02 -1.21648656e-02\n -9.22256336e-03 -3.37037668e-02 -3.55908275e-02  2.33340939e-03\n -1.76012330e-02 -2.97124521e-03 -1.11123724e-02 -2.45370772e-02\n -2.68994551e-03  1.18628889e-02 -2.65452154e-02 -4.81573073e-03\n  6.66899458e-02 -2.56377198e-02  4.25215438e-02 -1.68851949e-02\n -4.05320898e-03  2.78237574e-02  6.03862740e-02  4.23127860e-02\n -3.18863429e-03  4.89806607e-02  1.72499288e-02  2.01742910e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[118], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m silhouette_avg \u001B[38;5;241m=\u001B[39m \u001B[43msilhouette_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk_eigenvectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpseudo_labels\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    210\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    211\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    212\u001B[0m         )\n\u001B[0;32m    213\u001B[0m     ):\n\u001B[1;32m--> 214\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    224\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:130\u001B[0m, in \u001B[0;36msilhouette_score\u001B[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001B[0m\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m         X, labels \u001B[38;5;241m=\u001B[39m X[indices], labels[indices]\n\u001B[1;32m--> 130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(silhouette_samples(X, labels, metric\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    185\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    189\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:257\u001B[0m, in \u001B[0;36msilhouette_samples\u001B[1;34m(X, labels, metric, **kwds)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m    188\u001B[0m     {\n\u001B[0;32m    189\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    194\u001B[0m )\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msilhouette_samples\u001B[39m(X, labels, \u001B[38;5;241m*\u001B[39m, metric\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meuclidean\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m    196\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the Silhouette Coefficient for each sample.\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \n\u001B[0;32m    198\u001B[0m \u001B[38;5;124;03m    The Silhouette Coefficient is a measure of how well samples are clustered\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;124;03m       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\u001B[39;00m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 257\u001B[0m     X, labels \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Check for non-zero diagonal entries in precomputed distance matrix\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m metric \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecomputed\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1141\u001B[0m         estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[0;32m   1142\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1143\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1144\u001B[0m     )\n\u001B[1;32m-> 1146\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1148\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1149\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1151\u001B[0m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1152\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1153\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1162\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[0;32m   1164\u001B[0m check_consistent_length(X, y)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:938\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    936\u001B[0m     \u001B[38;5;66;03m# If input is 1D raise error\u001B[39;00m\n\u001B[0;32m    937\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 938\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    939\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124marray=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    940\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    941\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    942\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mif it contains a single sample.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[0;32m    943\u001B[0m         )\n\u001B[0;32m    945\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(array\u001B[38;5;241m.\u001B[39mdtype, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkind\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSV\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    946\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    947\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    948\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    949\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Expected 2D array, got 1D array instead:\narray=[ 3.42180841e-02 -1.06676053e-02  9.76189785e-03 -1.73727854e-03\n -5.90789914e-02 -1.84368331e-03  3.56702916e-02  1.39581482e-03\n  2.46395729e-02 -1.65780019e-02 -4.85542305e-02 -1.14951218e-02\n  6.57675089e-03 -2.73920428e-02  6.08483218e-02 -2.47640144e-02\n -1.55086080e-02  1.41540263e-03  1.04892654e-02 -3.47902328e-02\n -8.29673856e-02 -2.93387882e-02 -2.69389860e-02 -2.63558645e-02\n  3.87416594e-02  3.10202297e-02  6.06928580e-02  3.73603106e-02\n -5.28072705e-04  2.15034303e-03 -2.85482295e-02  8.57486855e-03\n -5.60640134e-02  3.59046762e-03  2.30839793e-02  1.33509394e-02\n -2.87624132e-02  4.74889129e-02  2.04882771e-02  4.59634885e-02\n  3.72267514e-02 -2.33419612e-02 -1.51904682e-02  3.63747440e-02\n  3.53081040e-02 -2.84880828e-02  2.65299324e-02 -2.76744775e-02\n  1.18166037e-01  3.81570943e-02 -2.77147666e-02 -2.37664282e-02\n  3.26162018e-03 -3.29441987e-02 -3.31442505e-02 -4.99936473e-03\n -2.97550634e-02  3.54483575e-02 -8.44130944e-03 -1.20985066e-03\n  3.43254432e-02 -7.36955777e-02  8.51478148e-03  1.95030142e-02\n -2.93148886e-02  2.28037611e-02  9.14611295e-03  5.74062653e-02\n -2.17369571e-02  7.79461069e-03  4.22228388e-02 -4.90873791e-02\n  2.94212662e-02 -4.48452942e-02  2.38211849e-03  1.11772493e-03\n -4.72990125e-02  1.30282473e-02  4.11145389e-02  5.70873581e-02\n -3.51253450e-02 -1.71891209e-02  2.87371501e-03  1.70566663e-02\n -1.03336656e-02  1.27872257e-02 -3.39109376e-02 -4.38410863e-02\n  1.79735869e-02  2.72876257e-03  1.87757667e-02  1.76043231e-02\n  2.57115569e-02 -4.22211885e-02 -1.95116252e-02 -3.34180356e-03\n  1.42156649e-02 -4.49861865e-03  3.60903777e-02 -1.50980661e-02\n  9.95518733e-03 -1.88651979e-02  3.70264798e-02  2.84435097e-02\n -3.71557474e-02  1.24887014e-02  2.64255591e-02  1.85980983e-02\n  5.37219038e-03  1.32010868e-02 -4.39589769e-02 -6.58578351e-02\n  4.54393029e-03  4.12329622e-02  2.95821279e-02  4.47114147e-02\n  3.89102213e-02  1.15194498e-02 -1.59769394e-02 -3.34593132e-02\n -4.26630899e-02 -2.83304304e-02 -3.40004475e-03 -2.63704341e-02\n  1.89394075e-02  4.50417548e-02  1.14878334e-04 -3.05763283e-03\n -2.15384960e-02  6.25467524e-02  6.42040893e-02  6.82053342e-02\n  6.18970441e-03 -5.02194017e-02 -2.70929746e-02 -3.84965725e-02\n  2.29256712e-02 -2.15272754e-02 -2.38366816e-02  5.08411322e-03\n  1.62763428e-02 -1.42969405e-02 -8.22864622e-02  9.03911665e-02\n -5.43838479e-02  6.78424025e-03  4.08152724e-03 -3.57618108e-02\n  1.16520850e-02  3.72144133e-02 -2.52784118e-02  1.54823381e-02\n  4.59182151e-02  8.42993520e-03 -2.88772788e-02 -1.88142750e-02\n -2.46940460e-02  1.09711336e-02  5.24007156e-02  3.34873646e-02\n  2.41335901e-03 -1.69386156e-02 -5.88801131e-02  3.84556279e-02\n  1.62078813e-02 -3.50344777e-02  1.22299064e-02 -2.25793514e-02\n  2.71182284e-02 -2.47923210e-02  5.11056324e-03 -3.03877406e-02\n -2.06495193e-03  1.80751141e-02  2.51253671e-03 -1.55187929e-02\n -1.95212907e-03  6.47532474e-03 -8.81842244e-03  5.77054285e-02\n  1.45415841e-02  5.69251403e-02 -1.24494852e-02  6.21847901e-03\n  1.71543267e-02  3.93836647e-02  3.19507867e-02  3.84223871e-02\n  1.11458063e-01  1.30681684e-02  2.10324340e-02 -4.69334349e-02\n  3.73610221e-02  8.40351917e-03 -3.42733189e-02 -1.78929307e-02\n  1.18498737e-02 -5.48488982e-02  2.92214546e-02  5.03540859e-02\n -2.98171230e-02 -2.10969206e-02  2.86634602e-02 -2.23388672e-02\n  3.04447953e-02  5.31621203e-02 -4.94061857e-02 -2.44240966e-02\n -1.79937191e-03 -1.16658537e-03  1.92979500e-02  3.44720073e-02\n -1.80062726e-02  2.52881963e-02 -9.90009308e-03  3.24260583e-03\n  6.70040026e-02  1.73029322e-02  5.05280197e-02 -6.32765144e-03\n -5.56908287e-02 -9.06402338e-03 -5.64708151e-02  4.39422950e-02\n -4.43273559e-02  1.07680745e-02  4.25554737e-02  2.39583962e-02\n -4.10647988e-02  7.72780599e-03 -1.19811669e-03  1.97420809e-02\n  6.57571331e-02 -1.15894573e-02 -7.24647194e-02 -1.21329017e-02\n  1.37987155e-02  3.37797450e-04 -1.93428658e-02  4.10099374e-03\n  2.64818650e-02  3.95532623e-02 -2.85804626e-02  3.56067858e-07\n  5.77828512e-02  3.72863300e-02 -3.88110057e-02 -2.56148800e-02\n  6.25245692e-03  1.54023441e-02 -1.75614879e-02  3.79495546e-02\n  3.72547247e-02  1.25374841e-02  5.49723841e-02  7.60745956e-03\n  4.18580808e-02  5.13962731e-02  9.40688979e-03 -1.38529437e-03\n -9.18448131e-05  9.43257660e-03 -2.46128105e-02 -5.40512837e-02\n  1.66218001e-02  1.52099878e-02 -5.94822541e-02  1.25577915e-02\n  1.62012167e-02  3.69100943e-02 -2.00299267e-02 -3.58892642e-02\n  1.90172307e-02 -2.61022132e-02  2.47226208e-02  2.51422338e-02\n  1.51010202e-02  1.46767255e-02 -3.15585323e-02 -1.70351053e-03\n -2.82006785e-02 -1.57529935e-02  3.46174054e-02  1.45949135e-02\n  3.95030268e-02  3.25984396e-02  1.13294711e-02  5.15526673e-03\n  1.07160211e-02  1.17506329e-02  4.83299866e-02 -4.25337330e-02\n  2.54633743e-02  3.31799663e-03  2.67793629e-02 -2.53778626e-03\n -1.70394313e-02 -1.66932940e-02  3.14745083e-02  1.89020429e-02\n  1.98835377e-02 -5.37078865e-02 -4.79939505e-02 -3.15622762e-02\n  4.76628318e-02  1.93342920e-02 -5.85542731e-02 -1.44373309e-02\n  8.05494841e-03  2.92822942e-02 -3.12911645e-02 -1.18430583e-02\n -4.37977873e-02  3.99416424e-02 -3.41954604e-02  3.68238986e-02\n  4.05534469e-02 -1.96357127e-02  1.80017054e-02  5.31825498e-02\n  1.90081801e-02  3.64036486e-02  2.94481404e-02  1.87339075e-02\n  6.55458644e-02  8.58169794e-03  6.82209507e-02  1.83735078e-03\n  2.16941182e-02  6.40307665e-02  3.34082916e-03  5.47589064e-02\n  1.31489364e-02 -1.34269958e-02 -1.22118564e-02 -2.89521758e-02\n  5.90809025e-02 -1.80458706e-02 -2.40770523e-02  4.01210561e-02\n -1.27519397e-02  6.90522417e-02 -2.71833390e-02  3.77944000e-02\n  2.15107333e-02 -6.09009378e-02 -5.46891838e-02  3.62452567e-02\n -4.99556027e-02  1.02656530e-02 -1.93464011e-02  3.26444060e-02\n  1.27624786e-02 -5.99939143e-03 -4.07625698e-02 -4.01184568e-03\n  1.36186117e-02 -4.13850183e-03  6.54283911e-03 -4.33826558e-02\n -5.57372384e-02 -5.90690784e-03 -1.19445911e-02  3.61640267e-02\n  1.05092174e-03  9.94909406e-02 -7.50018377e-03  2.59704366e-02\n -3.35371145e-03 -8.01269896e-03 -1.98051892e-02 -2.82358266e-02\n  1.50786787e-02  3.95017117e-03 -2.70488914e-02 -1.64064914e-02\n  7.32043525e-03  1.03483703e-02 -2.79483590e-02 -2.79650111e-02\n  2.61370391e-02  2.32125334e-02  6.01386465e-02  2.99330205e-02\n  1.50361918e-02 -1.16783222e-02 -3.54350619e-02 -9.53890197e-03\n -1.60268135e-02 -1.15521718e-02 -2.31285058e-02  1.65254101e-02\n -2.69472990e-02  1.42562911e-02  6.51261630e-03  2.81214509e-02\n -7.57706910e-02 -5.81581965e-02 -2.81328312e-03  2.03065239e-02\n -3.30131152e-03 -2.19425815e-03  3.69098261e-02  4.87924032e-02\n  3.26316018e-04 -6.70623314e-03  3.01186548e-04  2.25246530e-02\n  3.73362214e-03 -1.15546677e-02 -2.80533824e-02 -1.31824920e-02\n  6.40157610e-02 -7.52573758e-02 -2.87917280e-03  5.17578935e-03\n -3.66258458e-03  1.22846961e-02 -1.52258035e-02  4.62259687e-02\n -8.28521978e-03 -1.04054874e-02 -6.39904812e-02 -3.88383009e-02\n  1.96102969e-02  1.06616747e-02  1.55055514e-02  1.76628623e-02\n  2.19985703e-03  3.44655998e-02 -2.43697055e-02  4.24890108e-02\n -1.04218740e-02 -1.18980138e-02 -8.90897680e-03 -3.88931446e-02\n  4.49360237e-02 -9.32201091e-03 -1.26756392e-02  6.11680932e-03\n -1.19516980e-02 -3.91737837e-03 -3.62362973e-02 -6.13267161e-02\n  2.80768946e-02  2.19069049e-02 -1.32054882e-02  7.55391875e-03\n  1.85139233e-03 -4.61443812e-02 -3.78352962e-02 -1.06795793e-02\n -1.21175218e-03  4.19750251e-03 -9.55233816e-03 -2.03875713e-02\n  5.89707447e-03 -2.27986667e-02 -7.60537805e-03 -2.96444949e-02\n -3.23305950e-02  4.08668444e-02  2.27842703e-02 -4.75452580e-02\n -8.15098581e-04  1.11472495e-02 -1.20180594e-02 -2.85449494e-02\n  2.65816282e-02  9.82705597e-03 -3.19459885e-02 -2.22500786e-02\n  5.23840599e-02 -1.41348587e-02  1.78180709e-02  2.44422965e-02\n  2.99739242e-02 -4.02688682e-02  1.25280360e-03 -9.15325657e-02\n  6.86597750e-02  6.60150312e-03  3.76362610e-03  9.66899749e-03\n  1.88873503e-02  5.52567281e-03  2.80663334e-02 -2.94073950e-03\n  2.64876150e-02 -4.97503253e-03 -1.48841403e-02 -1.61051024e-02\n  2.50087474e-02  1.09365042e-02  1.45816626e-02  3.24027725e-02\n -6.57506706e-03 -2.48535201e-02  2.91602500e-02 -5.69668319e-03\n -4.85454127e-02 -2.25796923e-03 -2.85186134e-02 -1.55459382e-02\n -2.35996637e-02  2.39815027e-03  3.05724815e-02  5.41846361e-03\n  3.58869396e-02  5.07669337e-03  1.39000593e-03  3.76666361e-03\n  3.72482166e-02 -1.59859117e-02  3.23143192e-02  1.62717197e-02\n  4.96610552e-02  1.37279416e-02 -4.75539416e-02 -2.35693902e-03\n  4.01398167e-02  3.82967177e-03 -1.46265039e-02  8.99556279e-03\n  1.47170911e-03  1.66722722e-02  2.58590020e-02  9.08292644e-03\n -1.36549177e-03  1.42582897e-02 -3.90860587e-02  2.30081864e-02\n  1.68408565e-02  8.12727399e-03  4.27600481e-02  1.64819416e-02\n  1.61629748e-02 -8.36795941e-03 -2.47453470e-02 -1.24778841e-02\n  4.35280502e-02  2.32250411e-02  9.70627181e-03 -2.17560660e-02\n  2.37768441e-02 -8.35516118e-03  7.45645911e-03  1.02210259e-02\n  2.37595383e-02 -1.09368637e-02 -2.33560405e-03 -6.46449951e-03\n -5.24342107e-03 -1.59829087e-03 -6.51947856e-02 -3.01125962e-02\n -2.41542105e-02 -3.88235897e-02 -2.25255713e-02 -4.23029345e-03\n  1.64969862e-02 -5.01862392e-02 -5.07756472e-02 -3.17684934e-02\n  3.06716599e-02  3.77647243e-02  7.25280270e-02 -6.11166330e-03\n -8.05715856e-04 -4.08594543e-03 -3.67439166e-02 -4.18000035e-02\n  3.90075631e-02  4.89491783e-02 -1.17698535e-02 -4.61592712e-02\n -7.25825354e-02 -2.79057026e-02 -3.07498313e-02 -1.29949432e-02\n  2.33776327e-02 -2.11613178e-02  1.13216536e-02  2.88581848e-03\n -1.47881145e-02  1.20132929e-02 -7.31711835e-02 -2.34729238e-02\n  1.42925279e-02  4.60184403e-02  3.01881433e-02 -1.22445989e-02\n -2.71292496e-02 -3.24998889e-03 -2.80264616e-02  2.76431795e-02\n  8.16543400e-03 -8.55214149e-03  8.08408111e-03  2.57462524e-02\n  1.78627297e-02  1.87428650e-02 -1.49848191e-02 -6.33278303e-03\n  2.49050539e-02  6.40120870e-03 -2.62037758e-02 -3.51277888e-02\n -2.71785408e-02 -8.95567797e-03 -5.31988330e-02 -3.33282016e-02\n  4.20386158e-02 -4.17411588e-02  2.58229170e-02  3.45692299e-02\n  7.26388115e-03  1.13006309e-03  1.41074881e-02  1.51159137e-03\n -8.18297081e-03  3.74371782e-02 -6.41784593e-02 -1.82465538e-02\n -3.48709412e-02 -5.13107330e-02 -2.71874182e-02  1.21279480e-02\n  1.54595459e-02  4.14679497e-02  5.33081554e-02 -2.36858567e-03\n  6.06442466e-02 -3.45230699e-02  2.72924043e-02 -2.47241522e-04\n -3.78012024e-02 -1.75626334e-02  3.59302089e-02  2.50857789e-02\n -1.15140760e-02 -1.55295040e-02 -2.89102248e-03 -2.17212643e-02\n  4.16025147e-02  6.49670139e-03 -1.41804656e-02 -7.76072405e-03\n -2.81620473e-02 -1.95729136e-02 -2.77689509e-02 -2.11002436e-02\n  3.57708856e-02 -2.80954186e-02  4.34520468e-02  2.54419558e-02\n  1.93968019e-03  1.74011383e-02 -4.24527079e-02 -4.87458184e-02\n -4.20696661e-02 -3.65938693e-02 -3.56470682e-02 -1.93592999e-02\n  2.07568239e-02  1.73984829e-03 -5.27157895e-02  5.53154983e-02\n -2.41011530e-02  4.03166339e-02 -8.87010526e-03  1.21322721e-02\n -2.68715043e-02  4.78688069e-02  1.29871489e-02  1.38725089e-02\n  2.99002733e-02  3.46896090e-02  8.47313262e-04  2.21407600e-03\n -6.64938837e-02 -8.84330831e-03 -3.09231374e-02 -6.03915018e-04\n  3.20547298e-02 -7.81039102e-03  2.87739746e-02  3.05583496e-02\n  3.16287540e-02  8.56130291e-03  1.61118787e-02 -3.24605815e-02\n  3.39283161e-02 -5.41956201e-02  1.02180773e-02  3.53976563e-02\n  1.10155968e-02  1.68368313e-03 -2.18614694e-02 -6.73281178e-02\n -6.45067403e-03  3.33111244e-03  5.93389757e-02 -1.04912920e-02\n  2.90363636e-02  1.40792551e-02  6.30254578e-03  3.58664058e-02\n -2.53124479e-02  2.25652326e-02 -5.77359111e-04 -2.92755221e-03\n  3.63229169e-03  3.90454605e-02  9.51006822e-03 -3.24643962e-02\n -1.34666869e-02 -3.45175527e-02 -2.06782725e-02 -1.19680148e-02\n -7.34838173e-02  1.04327267e-03 -2.27143411e-02 -5.50386403e-03\n  2.59204339e-02 -2.13630330e-02 -4.74938229e-02 -9.48234461e-03\n  7.44100511e-02  7.08385110e-02  6.32095477e-03  2.66469698e-02\n  3.38760717e-03 -3.90653964e-03  1.16159860e-03 -3.13857459e-02\n -9.03026480e-03  2.65456680e-02 -8.78712069e-03 -1.14415807e-03\n  1.17063522e-03 -1.92478839e-02  3.68883945e-02 -1.62639469e-02\n -4.19347845e-02  4.35140952e-02 -3.73110287e-02  1.23325270e-02\n  2.19130181e-02  2.56084371e-02  7.25280046e-02  2.91179121e-02\n  8.36846828e-02 -4.35659029e-02 -8.35609250e-03  4.58607404e-03\n -5.59556996e-03  2.92071961e-02  3.93319428e-02  3.55720185e-02\n  1.98808759e-02 -4.54112738e-02  1.48039786e-02 -8.02166201e-03\n  1.59264961e-03 -8.39544367e-03 -3.28469537e-02  2.47576684e-02\n -7.61143193e-02 -2.08254084e-02  1.62602682e-02 -3.04639358e-02\n -2.17193533e-02 -4.43004072e-03 -1.52567113e-02 -4.84847687e-02\n -1.17601827e-02 -6.35271370e-02  4.24793269e-03 -2.31463071e-02\n -3.50244896e-04  1.86845157e-02  3.22816782e-02 -3.97873893e-02\n  3.62577848e-02 -4.68123741e-02 -1.79854073e-02  1.38707738e-03\n -2.24105828e-02  2.02228073e-02 -5.49124181e-02 -7.16325045e-02\n  4.79527004e-02 -4.67304103e-02  4.06398997e-02 -3.20381075e-02\n  5.60447760e-03  2.57881987e-03 -1.82499718e-02  5.19588590e-03\n  3.32999462e-03  2.25398857e-02  4.96628843e-02 -2.00414062e-02\n -2.90098302e-02  1.59345306e-02  3.59970927e-02  9.10980976e-04\n -2.53395867e-02  9.97680426e-03 -1.40363816e-02 -1.36225969e-02\n -3.99951404e-03  1.35265971e-02  1.67716690e-03 -5.48022129e-02\n -1.25386287e-02 -1.07047446e-02  3.30658443e-02 -1.76461320e-02\n  2.14501042e-02  3.02644689e-02 -5.83577296e-03 -1.96602382e-02\n -5.12638241e-02  2.26401333e-02 -1.32676661e-02  4.28262353e-02\n  4.36481386e-02  9.91506130e-03 -8.45065154e-03  1.79862715e-02\n -2.76043061e-02 -2.49280930e-02 -2.93538496e-02 -3.83058265e-02\n -3.74020264e-02  8.24281422e-04 -4.81557380e-03  1.30771264e-03\n  9.49643552e-03 -5.19729033e-02  3.81641500e-02  3.11720483e-02\n -1.15101235e-02 -2.94274278e-02  4.05922905e-03 -2.26000734e-02\n -2.03646980e-02 -9.22777504e-02 -1.85722522e-02  3.37464958e-02\n -5.15015125e-02  8.89871642e-03 -7.36175478e-03  8.27680808e-03\n -2.28725560e-02  3.26822363e-02  1.26480311e-02  2.26432066e-02\n -3.88532020e-02 -3.69665883e-02  1.44297518e-02 -1.54920481e-03\n  2.33350229e-03 -2.54025850e-02  2.36255154e-02 -2.74214577e-02\n  1.85194947e-02  5.97900935e-02 -5.90597838e-02 -3.35657853e-03\n -4.43326496e-02  2.10121106e-02 -5.84871322e-02  4.50278297e-02\n  5.39775118e-02 -3.43936197e-02  2.50951177e-03  1.19626289e-02\n -8.08885135e-03  2.10621506e-02 -3.95104773e-02  3.03309802e-02\n  5.23507893e-02 -3.81095819e-02 -7.63536990e-02 -2.54498273e-02\n -2.41915062e-02  9.57219221e-04  1.70935895e-02 -7.26494240e-03\n -1.71846244e-02 -4.07149903e-02  1.44807203e-02 -3.40680592e-02\n -1.02731222e-02 -3.74081507e-02 -1.15459282e-02 -1.65656619e-02\n  5.19147180e-02 -5.08948006e-02  1.14877324e-03 -4.33650352e-02\n -2.82994658e-02 -3.09068039e-02  3.38300364e-03 -3.97522897e-02\n  3.37059572e-02 -3.29212993e-02 -1.82965286e-02 -8.00489113e-02\n -8.56158584e-02 -3.00997086e-02 -4.18349169e-02 -1.06652146e-02\n -1.41634513e-02  4.86187041e-02 -5.79449534e-02 -3.26083899e-02\n -2.69033690e-03  2.53589619e-02 -2.50752177e-02  2.97869090e-02\n  6.95786923e-02  2.30584992e-03 -1.33689465e-02  2.21960526e-02\n -2.12461632e-02 -6.95791095e-02 -4.12467942e-02  7.95270875e-02\n -6.32799342e-02  7.43849948e-03 -4.88964394e-02  5.50187891e-03\n -2.40201633e-02 -2.19667014e-02  5.45400241e-03  1.39101334e-02\n -2.81435028e-02  4.82508205e-02 -5.28882677e-03 -5.40093984e-03\n  1.67793632e-02 -1.00305472e-02  6.03833003e-04 -4.78425669e-03\n -1.53062753e-02  4.09782231e-02  2.98490282e-02  1.83052961e-02\n -4.79068607e-02 -2.59555597e-02 -2.86078430e-03 -1.04995100e-02\n -1.04182782e-02  1.88700687e-02  3.67380902e-02 -2.92639118e-02\n -4.41675335e-02  1.61481043e-03 -1.91482399e-02  1.47453621e-02\n  2.52718665e-02  1.11932578e-02  6.92534400e-03  4.01024930e-02\n  2.76672114e-02 -3.71680893e-02  2.32658423e-02  4.40677702e-02\n -1.45099568e-03  2.59655938e-02  3.54468375e-02 -1.21648656e-02\n -9.22256336e-03 -3.37037668e-02 -3.55908275e-02  2.33340939e-03\n -1.76012330e-02 -2.97124521e-03 -1.11123724e-02 -2.45370772e-02\n -2.68994551e-03  1.18628889e-02 -2.65452154e-02 -4.81573073e-03\n  6.66899458e-02 -2.56377198e-02  4.25215438e-02 -1.68851949e-02\n -4.05320898e-03  2.78237574e-02  6.03862740e-02  4.23127860e-02\n -3.18863429e-03  4.89806607e-02  1.72499288e-02  2.01742910e-02].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "silhouette_avg = silhouette_score(k_eigenvectors, pseudo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5c7120ee577008"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
